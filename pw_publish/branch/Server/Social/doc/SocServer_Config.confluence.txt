Здесь будет жить инфа по "социальному серверу" (предназначен для обработки действий пользователей "игрового лобби" -- а ля социальная игра, со зданиями, героями и т.п.)

h2. Общая архитектура

Текущий дизайн предусматривает реализацию "социального сервера" в виде отдельного массивно-параллельного кластера, включающего:
* Веб-сервера: (http-протокол, пишутся на питоне на базе tornado web server)
* Database: базы данных (MySQL или Postgres, возможно также использование быстрых нереляционных БД)
* Memcache: кэширующая прослойка (memcache или аналоги)
* Load Balancers: (nginx, для распределения нагрузки на веб-сервера)
* Координаторы: (отслеживают конфиг кластера и состояние его компонентов)


!Social_cluster_architecture.jpg|border=1!


h2. Deployment (Windows)

Основное дерево исходников (включающее необходимые под win32 tools) лежит в svn: https://svn.nivalnetwork.com/pw/trunk/Tools/SocialServer

h3. Установка (Windows)
Для установки компонент тестового кластера на win32-машине нужно выполнить следующие действия:
(болдом выделены названия соотв. компонент)

*Веб-сервер*,
*Координатор*
1. установить Python (рекомендуемая версия 2.6.*) http://www.python.org/ftp/python/2.6.6/python-2.6.6.msi
2. установить необходимые библиотеки для питона:
2.1. PyCURL: https://svn.nivalnetwork.com/pw/trunk/Tools/SocialServer/vendor/win32/pycurl-ssl-7.19.0.win32-py2.6.exe
2.2. mySQLdb: https://svn.nivalnetwork.com/pw/trunk/Tools/SocialServer/vendor/win32/MySQL-python-1.2.3c1.win32-py2.6.exe
3. слить основное дерево исходников из svn: https://svn.nivalnetwork.com/pw/trunk/Tools/SocialServer
4. при необходимости внести изменения в конфиг координатора (coord_cfg.py) и стартовые опции серверов/координатора (/batch/*.bat)

*Memcache*,
*Load Balancer*
1. достаточно слить из svn дерево https://svn.nivalnetwork.com/pw/trunk/Tools/SocialServer/tools, необходимые для win32 exe-шники и тестовые конфиги лежат прямо там.
2. при необходимости внести изменения в конфиг nginx (/tools/nginx/conf/nginx.conf) и стартовые опции memcache (tools/memcache/*.bat)
3. внести соответствующие изменения в конфиг координатора (coord_cfg.py) 

*Database*
1. установить MySQL Server (стандартный, бесплатный, Community Server) http://mysql.com/downloads/mysql/
2. завести одну или несколько mysql баз (типичные названия pw1, pw2 и т.п.) от лица mysql юзера с правами администратора (типичное имя pw)
3. применить к каждой базе структуру БД из https://svn.nivalnetwork.com/pw/trunk/Tools/SocialServer/tools/pw.sql
4. прописать адреса, имена баз, имена юзеров и пароли в конфиг координатора (coord_cfg.py)
5. завести набор тестовых логинов/паролей, запустив координатор с ключом "--create_test_logins=N"


h3. Порядок запуска (Windows)
Компоненты тестового кластера запускаются в следующем порядке:

1. Database(s): убеждаемся, что в системе запущен сервис MySQL
2. Memcache(s): запускаем Memcache-приложения (/tools/memcache/*.bat)
3. Load balancer: запускаем nginx (/tools/nginx/nginx.exe), убеждаемся в отсутствии свежих фатальных ошибок в /tools/nginx/logs/error.log
4. Координатор: запускаем /batch/ws_Coordinator.bat, убеждаемся в отсутствии фатальных ошибок на запуске (ERROR в консольном окне)
5. Web Server(s): запускаем /batch/ws_8801.bat (/batch/ws_8802.bat и т.п.)

Например, "модельный" тестовый кластер (load balancer + 2x memcache + 2x database + 2x web server + coordinator) запускается так:
{code}(install path)/tools/nginx/nginx.exe
(install path)/tools/memcache/mc8901.bat
(install path)/tools/memcache/mc8902.bat
(install path)/batch/ws_Coordinator.bat
(install path)/batch/ws_8801.bat
(install path)/batch/ws_8802.bat{code}

- при закрытии и перезапуске существующих компонент кластера (БД, memcache, web-servers) координатор должен справляться с апдейтом конфигурации кластера.
- при добавлении новых БД, memcache, web-servers требуется апдейт конфига координатора (coord_cfg.py) и перезапуск координатора; при добавлении web-servers требуется также апдейт конфига load balancer (nginx.conf) и его перезапуск (/tools/nginx/reload.bat)
- остановка компонент (кроме load balancer) выполняется простым закрытием соотв. консольного окна
- остановка load balancer (nginx) выполняется через /nginx/shutdown.bat (или удалением всех процессов nginx.exe через виндовый "диспетчер задач").


h3. Запуск тестового Web-Server (без memcache/database/load balancer)
Для того чтобы запустить Web-Server в тестовом режиме, без прочих компонент кластера, достаточно выполнить установку компонента Web Server (см. выше, раздел Установка) и запустить веб-сервер с опциями "\-\-coordinator=none \-\-mc=none \-\-sql=none" (именно это делает /batch/standalone_ws_8801.bat)
К такому веб-серверу нужно коннектиться напрямую на его собственный порт (в данном случае :8801), минуя load balancer.

Аналогично, можно запустить усеченную версию координатора (с параметрами "\-\-mc=none \-\-sql=none"), чтобы координатор не проверял наличие memcache/database.


h3. Конфигурация

h5. Конфиг координатора
В конфиге координатора задаются:
1. список веб-серверов, в виде COORDINATOR_WS_SERVERS = \{ "имя_сервера": \{параметры\}, ... \} (см. пример конфига для двух веб-серверов в svn)
2. список мемкэш-серверов, в виде COORDINATOR_MC_SERVER_LIST = \{ "ip:port", ... \} (см. пример конфига для двух memcache-серверов в svn)
3. список баз данных, в виде COORDINATOR_SQL_CONFIG = \[ \{ параметры БД 1 \}, ... \] (см. пример конфига для двух БД в svn)

Списки активных веб-серверов, активных мемкэш-серверов и доступных баз данных - апдейтятся всем веб-серверам, регистрирующимся после запуска на координаторе.

Координатор (пока) не умеет раздавать веб-серверам имена и порты. Координатор только проверяет отсутствие дубликатов "логических имен" и выдает серверам уникальные server_id. При этом полный список допустимых имен веб-серверов задается в конфиге координатора (coord_cfg.py) и конфиге load balancer (tools/nginx/conf/nginx.conf).

Также координатор (пока что) не умеет автоматически генерировать конфигурацию для load balancer (nginx.conf); сейчас нужно руками вписывать в nginx.conf имена/адреса веб-серверов, аналогичные указанным в конфиге координатора.


h5. Опции запуска координатора
Номер порта, на котором будет слушать координатор, задается опцией *\-\-port=<int>* (например --port=8700).
! именно этот номер порта, вместе с ip координатора, необходимо указать в опции запуска "\-\-coordinator=ip:port" для ВСЕХ веб-серверов "социального кластера".
Также желательно задать имя лог-файла: *\-\-log_file_prefix=<file_name>* (например \-\-log_file_prefix=logs/coord.log).

Для запусков в тестовом режиме можно отключить работу с реальным memcache опцией "*\-\-mc=none*",
и/или отключить работу с реальными БД (опцией "*\-\-sql=none*").

Опция *\-\-tick=<seconds>" задает периодичность опроса координатором компонентов кластера (каждые N секунд проверяем, что все заявленные компоненты активны == отвечают на запросы координатора).


h5. Опции запуска web server
Каждому веб-серверу необходимо задать опцию "*\-\-coordinator=ip:port*" (связываться с координатором на указанном ip:порту), 
либо "*\-\-coordinator=none*" (не связываться с координатором, работать в тестовом режиме).

Сейчас веб-сервера при запуске должны получать из командной строки номер порта и логическое имя веб-сервера (т.к. координатор пока не умеет раздавать имена серверов, он только контролирует уникальность имени сервера и наличие имени сервера в своем конфиге; после чего выдает уникальный server_id).
Номер собственного порта задается опцией *\-\-port=<int>* (например \-\-port=8801).
Имя веб-сервера задается задается опцией *\-\-server_name=<string>* (например \-\-server_name=one).

Также желательно задать имя лог-файла: *\-\-log_file_prefix=<file_name>* (например \-\-log_file_prefix=logs/ws1.log).

Для запусков в тестовом режиме можно отключить работу с реальным memcache и выбрать одну из заглушек опцией "*\-\-mc=none|mocky*". 
- заглушка "none" на все запросы к memcache отвечает отказом ("нет такого ключа", "не смогла установить ключ" и т.п.)
- заглушка "mocky", наоборот, на все запросы отвечает согласием ("да, юзер есть", "да, он ничейный" и т.п.)

Также для тестирования можно отключить работу веб-сервера с базами данных (опция "*\-\-sql=none*", здесь есть только один вариант заглушки).


h5. Опции запуска memcache
Каждому серверу memcache нужно задать:
- номер порта (например "*-p 8901*")
- количество памяти в мегабайтах (например "*-m 64*")
- макс. количество одновременных connections (например "*-c 1024*"): каждый веб-сервер кластера создает некоторое фиксированное кол-во соединений с memcache (обычно от 2 до 8 штук).


h5. Конфигурация load balancer (nginx)
Основное, что нужно задать в конфиге nginx load balancer - это набор веб-серверов, с их логическими именами ("one","two" и т.п.) и соотв. адресами/портами (со временем этот конфиг будет автоматически генерироваться координатором, но пока нужно расписывать nginx.conf вручную).
В текущем nginx.conf делается маппинг для двух рабочих веб-серверов (one и two). Для пустого суб-адреса ("/") делается случайный load balancing, для субадресов, совпадающих с именами веб-серверов ("/one", "/two") делается прямое проксирование на соотв. адрес/порт веб-сервера.


h3. Тестирование

Для прохождения юнит-теста отдельных компонент (без работающего кластера, чисто с заглушками) запускаем unit.test.py (5 тестов должно пройти OK)

Для прочих тестов нужно поднять кластер (см. "Порядок запуска" и "Конфигурация"), и затем:
- для прохождения юнит-тестов кластера запускаем cluster.test.py (6 тестов должно пройти OK)
- для тестирования matchmaking по http запускаем отдельным процессом mock_matchmaker.py (например \-\-port=8702), указываем тот же номер порта в конфиге координатора (COORDINATOR_PVX_MATCHMAKER), и запускаем test_async.py (можно это делать через "/batch/ws_MockMmClient.bat") 

