//
// Dan Vorobiev [started 26-01-2011]
//
=================================================================================

SOCIAL EXCHANGE

-- нашаманить id-шники pvp-юзеров
lobby.cfg
//  lobby_dev_start_id 1000

II
+ send_nl (no-login version)
+ receive(auid)
+ purge(msg_id,msg_id,..)
    + удалять из БД, потом из очереди
+ убрать вложенность msd_id->msg->fields

III
+ send.push
~ удаление по expire_time
~~ блокировка от быстрой повторной выборки (период)
~~ multi-send (распихиваем большой send_nl POST по нескольким адресатам project:auid)

IV
+ обвязка ws.login, ws.logout, ws.send
+ обвязка ws.onSend + забирание по keepalive (debug)
+ защита от повторного забирания сообщения с тем же msg_id 
    (сохранять в MD список обработанных msg_id, с тайстэмпом обработки, и чистить только через N времени?)
- #TODO: раскидать по отдельной табличке "почтовых хэндлеров"

ИЗМЕНЕНИЯ ПО СРАВНЕНИЮ С ДОКУМЕНТАЦИЕЙ:
~ для send желательно сделать не список сообщений, а словарь, с некими временными (чисто клиентскими) client_msg_id: 
    { <client_msg_id>:{..msg..}, <client_msg_id>:{..msg..}, }
  т.к. часть сообщений может пофейлиться, и удобнее вернуть список [failed client_msg_id], чем полностью содержание пофейленных сообщений
+ sender прижился в виде строки "<project>:<auid>", а не просто <int64 auid>


=================================================================================
=================================================================================

- сейчас получают открытый sntoken: 
    l_who
    l_login
    l_merge
    
!- l_merge получает временные токены (auid + pwtoken), запрос никак не подписан
!- l_join вообще не требует токена для главного auid, и запрос никак не подписан

PWCONNECT принимает (через nginx SSL дырку):
    'pwc_register': PWCRegisterAction, // регистрируем новый аккаунт
    'pwc_confirm': PWConfirmAction, // активируем аккаунт (отдаем полученный по email confirm код)
    'pwc_resend_confirm': PWRetryConfirmAction, // просим повторно выслать confirm код
    'pwc_change_email': PWCChangeEmailAction, // меняем email
    'pwc_change_pwd': PWChangePasswordAction, // меняем пароль
    'pwc_reset_pwd': PWResetPasswordAction // забыли пароль
 
WS принимает:
    l_who (snid=pwe, snuid=email_md5) -> auid=<auid>, token=challenge
    sn_login(pw#auid) обычный
    pwc_get_info // пока просто как unloggedActionMap['pwc_get_info'] = PWGetInfoAction

=================================================================================

PW CONNECT
+ [PWC/SSL] pwc_register( email, pwd, gender, ip )
    + isql.pwcRegister -> added ok
    + iaggregator.pwcRegister(..) -> auid
        + тестовая заглушка, чтобы пока что выдавало левый auid
    + генерируем confirm_token, isql.pwcPostRegister(confirm_token, auid)
    + отправляем по почте confirm_token
        + нужен thread-pool, т.к. smtp send адски медленно выполняется
        + нужен интерфейс ismtp 
            + реализация IRealSmtp
            + файловая заглушка IDummySmtp

+ AUTH( key=md5(sorted(params) + pwd_md5)
    + проверяем key 
    
+ [WS] pwc_who( email_md5 )
    + isql.pwcGet(old) -> pwd_md5, ..info..
    + пихаем в локальный token (?перенести в acc?)
    
+ [WS] pwc_login( email_md5, challenge, key(sorted(params)+pwd_md5) )
    + AUTH
    + обычный sublogin, pwc#email_md5
    
~~~~~~~~~

+ [WS and/or PWC] pwc_reset_pwd( email_md5 )
    + генерируем новый пароль
    + isql.pwcUpdatePwd(new) -> ok?
    + отправляем по почте pwd_new (+confirm_token, если не activated?)
    
+ [WS and/or PWC] pwc_confirm( email_md5, confirm_token )
    + isql.pwcActivate(..) -> activated ok
    
+ [WS] x/pwc_get_info( pwc_snuid ) 
    + отдаем email,gender
    + причем в формате протокола SA // {"error":<int>, "message":<string>, ...}

~~~~~~~~~

+ [PWC/SSL] pwc_change_email( email_md5, new_email(utf8) ), подпись pwd_md5
    + здеcь надо аккуратно раскидывать по шардам mysql 
        // на каждый pwc-аккаунт может быть 2 копии, с шардингом по email_md5 (для ланчера) и по pwc_snuid (для аггрегатора)
    ++ перейти на новую схему с постоянным pwc_snuid=<int64> и login="pwc#<pwc_snuid>" вместо "pwc#<email_md5>", 
        чтобы смена email ничего не ломала в аггрегаторе и монго

+ [PWC/SSL] pwc_change_pwd( email, new_pwd, key_md5 ), подпись старым pwd_md5
    + isql.pwcUpdatePwd(new) -> ok?
    
+ [PWC/SSL] pwc_retry_confirm (?), подпись pwd_md5
    + генерируем новый confirm token, прописываем в sql, отправляем юзеру по почте
    + isql.pwcUpdateOk() -> ok?
    
    
-- нужно пробивать новые дырки через nginx:
    - на WS-ки "только для SSL трафика" (обычный http сюда никак не должен попадать) -- сюда пойдут l_who и всё остальное, что может передавать sntoken
        -!!!- проверить безопасность операций merge/join и ввести там цифровые подписи
        - внедрить цифровые подписи на sn_login (token-то выдается случайный). вот только чем подписывать? 
            // можно, например, подписывать sntoken-ом (если мы его не передаем иначе как по SSL)
        - повесить на Вадима Г. таск по переводу l_who под SSL
    - на WS-ки от аггрегатора, чисто для pwc_get_info (сюда вообще никакой посторонний трафик не должен пролезать, только внутренний от SA)
    - на pwconnect (только SSL)



=================================================================================

ClusterAdmin
3HbFi%pPGVd^PTyv

=================================================================================

ТАСКИ АП8.1 
+ already logged on diff.server: нужно поддержать релогин на конкретный сервер
    + редирект уже пришедшего запроса на внутренний порт (не так травматично для клиента)
    
+ ModelData: пофиксить шаблон! (про сохранение next_uid кажется)
    
----!! конфигурация thrift_agent из координатора
    + разобраться, что там у Вадима не настроено, почему thrift-запросы на two не идут (оказывается, он пытался логин с решеткой, вида "fb#12345", по http передавать)

+ хранение nick/fame/fraction в аггрегаторе
    + IAggregator.setProperty
    + по получению subscribe или updateGameData сравниваем с имеющимися данными, и при необходимости делаем IAggregator.setProperty
    + заводим список с именами полей (nick/fame/fraction), высасываем из get_friends всё что есть по этому списку
        + int fame/fraction преобразуем из строчки (аггрегатор возвращает int property в виде строчки)

+ СТАТУСЫ в person_server 
    public enum FriendStatus
      ({
        None = 0,         //Не задан
        NotInGame = 1,    //Не в игре
        InCastle = 2,     //В Замке
        InBattle = 3,     //В битве
        InTraining = 4,   //В тренировке
        OnDragonRide = 5  //В походе на Горыныча
      })

 
# -------------------------------------------------------------------------------------------------------------
#~ sql_options = ClassDict( sql_host="localhost:3306", sql_base="pw1", sql_user="pw", sql_pass="pwdata" )
#~ reply = syncSqlCommand( u"INSERT INTO tbl_nick (`id`, `nick`) VALUES (16,'%s')" % unicode("Дан", 'cp1251'), sql_options )
#~ print "sql reply: %s" % reply


ТАСКИ АП8.1 
+ уникальность ников
X invalid heroes (?) 
    X может быть, просто отдельные инструкции в cluster.cfg: 
        invalid_hero aaa 
        invalid_hero bbb
+ конфиг-опция canReconnect (передавать в MM)
+ передавать с сервера параметры для ММ (ReconnectToMatchTimeout, AcceptPartyTimeout, NetworkDelayTime)
    "mmServerStatus": {
      "pvx_accept_timeout": <float>,
      "pvx_average_mm_time": <float>,
      "pvx_reconnect_timeout": <float>,
      "network_delay": <float>,
      "party_accept_timeout":<float>,
      "can_reconnect": 0/1,
    }



НАГРУЗКА
+ сократить избыточное логирование (http://devjira.nivalnetwork.com/browse/PF-34941)
+ МЕГА-ПРОБЛЕМА: дикие времена сериализации в json
+ МЕГА-ПРОБЛЕМА: накапливание heavy-операций блокирует прием пакетов (в т.ч. проверочных от координатора, что чревато развалом кластера)
-! ПРОБЛЕМА: очень большое время создания полной ModelData (25 героев, 800+ талантов, тысячи разнообразных вложенных вызовов; медленное общение с новым StaticData)
+ persistent guest 
    + понадобится и некий "guest relogin" (для guest указываем uid и "секретный флаг"?)
        + 1) .\wserver.py:482: if not (acc.guest or acc.is_failed): // достаточно отрубить аккаунту флаг guest, чтобы он начал сохраняться
        + 2) указать при последующем login "_allow_guest_relogin_=1"    
- подпереть в fsm-ках "already logged on different server", иначе не потянем с множеством WS
- single load сценарий:
    + login, auth/set_fraction, keepalive до упора (по счетчику), logout
    - насытить проверками fail-ов 
        ?- отдельный FailSafeContext?..
    - добавить логгинг времени, кол-ва CCU, кол-ва фейлов пакетов
- многократный load сценарий:
    - login(persistent guest) + auth + (set_fraction если N) + ping +   ...?  + logout + timeout_wait
    - повторить (до истечения случайного счетчика?)
- добавить каких-то операций с логикой (a la перекладывание талантов), keepalive
- добавить матчмейкинг
- добавить party-команды


=================================================================================

+ оставляем tsSaving при сохранениях
    + раздельный по mc/mongo
+ при фейлах сохранения mc и mongo: оставляем заявку на retry (после определенного таймаута)
+ в главном thread оставляем только общий таймаут на "успешное сохранение"; если по истечении таймаута имеем db=0, делаем дамп в файл

+ механизм cas восстанавливаем, но:
    + при фейле casS/addS все равно делаем gets, и смотрим: наш ли суффикс в конце строки ("::$<server_id>"); 
        + если суффикс наш, то грязно ругаемся, и восстанавливаем cas; если чужой, фейлимся (юзер от нас уплыл)

=================================================================================
                            
# -------- вариант с компрессией (zlib быстрее, bz2 сильнее жмет) -------- 
#~ json_length = len(db_json)
#~ compression_modules = dict( zlib=zlib ) # dict( bz2=bz2, zlib=zlib )
#~ for cname,comp in compression_modules.iteritems():
    #~ for level in [3]: # in xrange(1,10): 
        #~ t1 = time.time()
        #~ data = comp.compress( db_json, level )
        #~ t2 = time.time()
        #~ length = len(data)
        
        #~ #t3 = time.time()
        #~ #unpack = comp.decompress( data )
        #~ #t4 = time.time()
        #~ #info( "%s level %d compression time: %.3f, decompession time: %.3f, len %d (%.2f ratio)" % (cname, level, t2-t1, t4-t3, length, length*100.0/json_length) )
        
        #~ info( "%s level %d compression time: %.3f, len %d (%.2f ratio)" % (cname, level, t2-t1, length, length*100.0/json_length) )
        
        #~ dump_filename = "logs/dump_uid_%s_time_%d.%s.level%d.data" % (acc.uid, t1, cname, level)
        #~ dump_file = open( dump_filename, "wt" )
        #~ dump_file.write( data )
        #~ dump_file.close()

=================================================================================

#~ ################################
#~ ## отладочный вывод
#~ req_data = mc_reply["a1"]
#~ if req_data and len(req_data) == 3:
    #~ key, rdata, cid = req_data
#~ elif req_data and len(req_data) == 2:
    #~ key, rdata = req_data
#~ if rdata:
    #~ try:
        #~ data_dict = json.loads( stripAfter(rdata, "::") )
        #~ req_data = pathFind( data_dict, "db/db" ) or {}
    #~ except:
        #~ catch()
        
#~ if pers_data:
    #~ try:
        #~ data_dict = json.loads( stripAfter(pers_data, "::") )
        #~ pers_data = pathFind( data_dict, "db/db" ) or {}
    #~ except:
        #~ catch()

#~ info( "onUserDataCas: uid=%s, cas_id %s, acc %s..\n request data/db: %s \n reply data/db: %s" \
    #~ % (uid, cas_id, str(acc)[:1024], str(req_data)[:1024], str(pers_data)[:1024] ) )
#~ ################################


================================================================================
[26 авг] FEATURE FREEZE (по партиям френдам и т.п.)

================================================================================
компрессия: думаю, zlib level 2-3 вполне себе оптимален

[2011-09-09 13:25:23,539     INFO] zlib level 1 compression time: 0.002, decompession time: 0.001, len 20979 (12.05 ratio)
[2011-09-09 13:25:23,542     INFO] zlib level 2 compression time: 0.003, decompession time: 0.000, len 20554 (11.80 ratio)
[2011-09-09 13:25:23,548     INFO] zlib level 3 compression time: 0.002, decompession time: 0.001, len 19931 (11.45 ratio)
[2011-09-09 13:25:23,569     INFO] zlib level 4 compression time: 0.004, decompession time: 0.000, len 18553 (10.65 ratio)
[2011-09-09 13:25:23,575     INFO] zlib level 5 compression time: 0.004, decompession time: 0.000, len 18379 (10.55 ratio)
[2011-09-09 13:25:23,585     INFO] zlib level 6 compression time: 0.008, decompession time: 0.000, len 18158 (10.43 ratio)
[2011-09-09 13:25:23,594     INFO] zlib level 7 compression time: 0.007, decompession time: 0.000, len 18015 (10.35 ratio)
[2011-09-09 13:25:23,612     INFO] zlib level 8 compression time: 0.018, decompession time: 0.000, len 17647 (10.13 ratio)
[2011-09-09 13:25:23,641     INFO] zlib level 9 compression time: 0.026, decompession time: 0.000, len 17449 (10.02 ratio)

[2011-09-09 13:25:23,677     INFO] bz2 level 1 compression time: 0.032, decompession time: 0.003, len 13314 (7.65 ratio)
[2011-09-09 13:25:23,719     INFO] bz2 level 2 compression time: 0.037, decompession time: 0.003, len 12758 (7.33 ratio)
[2011-09-09 13:25:23,762     INFO] bz2 level 3 compression time: 0.037, decompession time: 0.004, len 12758 (7.33 ratio)
[2011-09-09 13:25:23,802     INFO] bz2 level 4 compression time: 0.034, decompession time: 0.004, len 12758 (7.33 ratio)
[2011-09-09 13:25:23,842     INFO] bz2 level 5 compression time: 0.035, decompession time: 0.004, len 12758 (7.33 ratio)
[2011-09-09 13:25:23,884     INFO] bz2 level 6 compression time: 0.036, decompession time: 0.003, len 12758 (7.33 ratio)
[2011-09-09 13:25:23,924     INFO] bz2 level 7 compression time: 0.035, decompession time: 0.003, len 12758 (7.33 ratio)
[2011-09-09 13:25:23,966     INFO] bz2 level 8 compression time: 0.036, decompession time: 0.003, len 12758 (7.33 ratio)
[2011-09-09 13:25:24,005     INFO] bz2 level 9 compression time: 0.034, decompession time: 0.003, len 12758 (7.33 ratio)

=================================================================================
для конфы: 

{expand:<title>}
..тело блока
{expand}

=================================================================================

USE CASES В ПАРТИЯХ/ФРЕНДАХ:
- все члены партии должны быть за одну сторону (fraction), неопределившихся пригласить нельзя (?)
    + пока подперто в соц.клиенте, который показывает друзей только за свою фракцию
+ в процессе матчмейкинга нельзя выполнять invite/accept 
+ команды kick/disband должны вышибать всю группу из матчмейкинга (party_cancel?..)
    + logout порождает автоматический disband, так что этот случай по идее будет подперт
+ если юзер в матчмейкинге, пригласить его нельзя (и сам он не может приглашать), отлуп "юзер в матчмейкинге"

+ при приглашении мастеру приходит create_init, а потом еще и полный список model changes в процессе create_init
- в model changes никак не отражаются ошибки инвайтов и т.п. (cmd="error")
+ нет удаления инвайта при его таймауте

=================================================================================

+ добавить к FriendData (возможно, через механизм set_edge_property/get_edge_property(self,self)):
    "fame", "fraction", "nickName"
    + nickname в model changes попадает в странной кодировке (quoted-printable)
        
+ подпереть апдейт инфы из Person всем френдам
    + UC: разослать статус online=1 всем, кого это касается
    + UC: проапдейтить увеличенный после сессии Fame
    
+ меняем параметры party.invite с invite_snid/invite_snuid на invite_auid (==person_uid)
+ подпираем op=init в model changes

+ рассылаем party_go

=================================================================================

+ хотелось бы уметь накапливать model changes, идущие подряд и относящиеся к одному объекту (пути), в одной строчке model change
    (для случаев, когда мы заполнеям поля свежесозданного объекта, при инициализации)

=================================================================================
PERSON SERVER
=================================================================================
+ координатор знает адрес PersonServer, апдейтит его WS-кам
+ координатор апдейтит PersonServer'у текущую конфигурацию WS-ок (чтобы тот знал, кому слать party-команды)
    + пока только при старте/обнаружении координатором нового peer-сервера (а надо бы и на старте координатора/PersonServer тоже)
        + надо продумать механизм старта Coordinator + PersonServer + WorkServers (start_all..)
            + сообщим координатору, что мы стартовали (pers_ready), он в ответ пришлет полный peers {}
?- координатор проверяет статус PersonServer, ругается если он не отвечает 

+ для работы с партиями PersonServer надо научить смотреть, на каком WS (if any) залогинен юзер; 
    + что если просто полагаться на статус online/offline по subscribe? :)
    
+ доставить список френдов юзеру (через pending_events)
    
---------------------------------------------
+ STAGE I : прокинуть party_invite + party_accept через PersonServer, от одного конечного клиента к другому, до успешного создания партии
    + WS: "action=party&cmd=invite" -> PartyAction -> send2pers "action=party&cmd=invite"
        - если на http нам ничего не ответили, докладываем юзеру server_party&error="person server 404"
    + pers: "party&invite" -> onPartyInvite, !PartyContext.master, выдаем party_id -> send "server_party&invite" (party_id, members..) 
        + PartyContext.master, выдаем party_id 
        + или отлуп "wannabe-мастеру" ("server_party&error")
    + WS: "server_party&cmd=invite" -> onPersonServerPartyCommand -> ping reply "server_party&invite"
        + поправить from_user_data (сейчас там ненужная служебная фигня)
    + WS: "server_party&cmd=error" -> onPersonServerPartyError -> ping reply "server_party&error"
    
    + WS: "party&accept" -> PartyAction -> send2pers "party&accept"
    + pers: "party&accept" -> onPartyAccept, добавляем в PartyContext, рассылаем всем server_party&join с новым членом партии
        + или отлуп "wannabe-джойнеру" ("server_party&error: таймаут, нет приглашения, нет такой группы.. и т.п.")
        
    + сделать параметр для "мгновенного ответа" на polling PING (например, "сколько секунд мы готовы ждать", 0 чтобы ответ был мгновенный)
    
    + придумать, как бы для отладки (и вообще стабильности) дублировать subscribe для релогинов, при перезапуске PS 
        + action=ps_resubscribe // можно, например, чтобы WS высылали subscribe для всех своих юзеров, по спец.запросу PS после его рестарта
        
    + pers: время от времени, по таймеру, приглашения протухают
        + отсылаем мастеру "server_party&invite_timeout"
        + если это было последнее приглашение (у него всегда увеличенный таймаут?), прибиваем PartyContext ("server_party&party_expired")
        
---------------------------------------------
+ STAGE II: остальные команды 
    + сейчас партия существует вечно (пока бегает PersonServer)
        + по unsubscribe нужно, наверное, выкидывать из партии
        + нужна, вероятно, команда "проверить статус партии" == получить полный список members, с userinfo-й
        ? в принципе, никто не мешает нам держать PartyContext вплоть до disband (в т.ч. при перезаходе юзеров)
            - только надо настроить редкий (раз в N минут) запрос "жив ли там еще PS / жива ли моя партия", 
                чтобы при гибели или рестарте PS соц.клиент мог сбросить партию
        + можно прокинуть disband, а "вечные партии" пока оставить на совести PS
        + tick должен проверять и партии без pending_invites
        
    + disband 
        + миграция лидера при его disband-е (4ч)
    + kick (2.5ч+1ч) во "френдах" и "клиентских party-командах" соотв.
    + decline
    
    + choose_hero (broadcast всей партии) -- игроки выбирает героев для party matchmaking
        + добавить hero_level 
        + если есть локальные несоответствия (юзер успел кликнуть choose_hero, но тут пришел party_go с предыдущим героем) юзер идет лесом (получает pending warn?),
            при этом в матчмейкер должен уйти герой, указанный мастером 
    
+ GO (4ч)
        + лидер видит список героев, go можно посылать только когда список героев полный (у всех персон выбран party_hero)
        + лидер в момент go явно рассылает свой список members/heroes
            + WS-ки по получению party_go отсылают mm_add в сторону матчмейкера, юзеру ставим pending извещение, что матчмейкинг пошел (и теперь надо его mm_ping)
                + утрясти с Денисом, в каком формате он хочет список членов партии (и какие именно UID туда включить)
                    + хочет party_id + список zz_uid(=auid) -- пока сделал просто строчку с запеченным списком auid-ов, через запятую
                + PS тоже придется известить, что у нас стартовал MM (иначе непонятно, разрешать новый party_go или нет)
                    + party.progress (1), reason=party.go
            + в худшем случае -- например, нет такого героя, юзер его удалил? или юзер упал с сервака? -- происходит fail party_go
                + т.е. отправляем для PS party.progress 0 (ну и можем попутно уточнить причину, мол, "reason"="нет такого героя")
            + fail матчмейкинга надо как-то обрабатывать для партии (извещать лидера, что снова можно party_go?)
                + т.е. если у нас случился mm_cancel, и есть party_id, отправляем для PS party.progress 0 ("reason"="mm_cancel")
    
    + добавить механизм упорядочивания сообщений по времени 
        + можно тупо присваивать ++msg_id каждому ответу, который тем или иным способом попадает в пользовательский acc.pending_events
        
    + сделал хелперы для рекурсивного unquote и unjson (для содержимого переправляемых на клиент party-евентов, чтобы там не было лишних '\"' и "%20" в юзеринфе)

+ не отлавливаем случай, когда юзер ушел с сервера (offline) 
    + нужно выкидывать его из партии (или хотя бы смотреть его acc.online), иначе спокойно проходит party.go
    
?- сейчас мы заставляем WS ждать, пока аггрегатор не отдаст нам список френдов;
    ? лучше сделать асинхронную схему, когда апдейты френдов (начальный и периодические) будут доставляться на WS отдельным направленным send-ом из PS
    

-----------------------------------------------------
- TEST STAGE IIA: полный авто-тест операций с партиями

- invite
    - несуществующего юзера -- отлуп
    - существующего, но offline юзера -- отлуп
    - юзера не в партии 
    - юзера во временной партии (один мастер)
    - юзера в постоянной партии -- отлуп
    - инвайт, когда мы без партии
    - инвайт, когда мы мастер
    - инвайт, когда мы не мастер -- отлуп
    - повторный инвайт своего же мембера -- отлуп
    - двукратный инвайт одного юзера
    - инвайт себя -- отлуп
    - инвайт юзера, у которого идет матчмейкинг -- отлуп
    
- accept
    - существующего инвайта
    - несуществующего инвайта -- отлуп
    - существующего, но протухшего (таймаут) -- отлуп
    - есть временная группа 
    - аксепт второго (корректного) инвайта подряд -- отлуп
    - после того как нам отменили инвайт (через kick) -- отлуп
    - после логаута (ждем до bad session) и релогина -- отлуп
    - после того как мы стартовали матчмейкинг -- отлуп

- decline
    - существующего инвайта
    - несуществующего инвайта -- отлуп
    - существующего, но протухшего (таймаут) -- отлуп
    - есть временная группа 
    - после того как нам отменили инвайт (через kick) -- отлуп
    - после логаута (ждем до bad session) и релогина -- отлуп
    - после того как мы стартовали матчмейкинг -- отлуп
        
- disband
    - у нас нет группы -- отлуп
    - есть временная группа
    - есть постоянная группа
    - есть инвайт, но нет группы -- отлуп
    - после логаута (ждем до bad session) и релогина -- отлуп
    - после party.go: матчмейкинг у всей группы должен остановиться
    
- kick
    - у нас нет группы -- отлуп
    - kick себя -- отлуп
    - есть временная группа (мы мастер), кикаем инвайт
    - есть постоянная группа (мы не мастер) -- отлуп
    - есть постоянная группа (мы мастер), кикаем мембера
    - есть постоянная группа (мы мастер), кикаем инвайт
    - есть постоянная группа (мы мастер), кикаем несуществующего юзера 
    - есть постоянная группа (мы мастер), кикаем мембера второй раз подряд -- отлуп
    - есть постоянная группа (мы мастер), кикаем инвайт второй раз -- отлуп
    - после логаута (ждем до bad session) и релогина -- отлуп
    - после party.go: матчмейкинг у всей группы должен остановиться
    
- hero
    - неправильный герой (плохой id) -- отлуп
    - нет группы -- отлуп
    - есть инвайты, но нет группы -- отлуп
    - временная группа
    - постоянная группа
        
- go
    - нет группы -- отлуп
    - временная группа -- отлуп (нет мемберов! делай disband и юзай обычный матчмейкинг)
    - кто-то из юзеров не выбрал hero -- отлуп
    - кто-то из юзеров находится в матчмейкинге (например, не успел вернуться из предыдущего party go) -- отлуп

- старт обычного матчмейкинга (mm_add)
    - если есть группа -- отлуп
    - если есть инвайт(ы)
    
- когда кто-то покидает группу по logout (disband:offline)
    - после party.go: матчмейкинг у всей группы должен остановиться
    - временная группа, ушел последний приглашенный: группа должна развалиться
    - постоянная группа, нет инвайтов, ушел последний мембер: группа должна развалиться

    
---------------------------------------------
- STAGE III: ModelData-подобные структуры 
    - 8ч friends + 8ч party
        
---------------------------------------------
- изменения по PartyData:
    ------ members и pending объединяются в один Set; на PartyMember(Person) появляется bool is_pending
    - PartyMember также аггрегирует hero_id, hero_crc, hero_level
    - disband, возможно, реализуется как kick(self)
    + возможен kick(pending_invites) // типа, отменяем инвайт
    
    
? фильтр для action=party&cmd=xxx на WS делаем? (минимальный?)
    ? мастер или без партии
        invite
        accept
        decline
    ? мастер или member
        disband (reason=client|technical) -- при unsubcribe порождаем автоматический disband (а decline?.. -> обойдутся таймаутом)
    ? только мастер
        kick 
        go
            
---------------------------------------------
+ x?action=server_party&cmd=<cmd>&to_uids=<uid,uid,uid..>& more params...
    
+ загадочное рассыпание wserver на IUserData -- зависело от порядка наследования интерфейсов (попадали на пустой IUserData.create -> NotImplemented)
  File "C:\Work\Tornado\tornado\modeldata\datamanager.py", line 86, in loadModel
    acc.model.create( xdb, create_version=u"", create_heroes=0 )
  File "C:\Work\Tornado\tornado\iuserdata.py", line 24, in create
    raise NotImplementedError()
NotImplementedError

---------------------------------------------
- придумали новые статусы для party-матчмейкинга
    - 49: я прислал mm_add (после party_go), но кто-то из членов партии еще не прислал (50 будет означать, что вся партия уже в матчмейкинге)
    - ? 201: кто-то из членов партии пофейлил таймаут на accept
    - ? 202: кто-то из членов партии сделал mm_cancel

=================================================================================
NEW MATCHMAKING STATUSES
=================================================================================
+ В соц.сервере должен быть таймаут на переход из 1 в 50. Он сработает в случае недоступности сервиса matchmaking’а. Таймаут контролируется социальным сервером (не клиентом).
+ Убираем подпорку “Progress 51” из соц. сервера. Заменяем ее на новый статус AcceptAccepted (103). Этот статус выставляется самостоятельно соц. сервером поверх статуса WaitingAccept (100) сразу после приема “mm_accept”
+ Сделать sessionKey одноразовым. При создании sessionKey в логин-сервер передается флаг «нужен одноразовый ключ», Remove для таких ключей не вызывается. Ключ удаляется сразу после успешного логина по нему.

- По статусу ClientDropped (107) позволяем клиенту прислать mm_reconnect или mm_leave (на АП7 будет работать только mm_leave)
    + заводим команды в matchmaking.py
    + протаскиваем "reconnect" и "leave" до Gateway
    -[C++] на gateway мапим "reconnect" и "leave" на соотв. методы RPC
    ? обсудить вопрос с подтверждением mm_reconnect и mm_leave (должен видимо появляться какой-то статус в ответе соц.сервера, аналогично 103?)
        + например, ставим "prediction" статус confirm=[CONFIRM_NEW, CONFIRM_ACCEPT, CONFIRM_RECONNECT, CONFIRM_LEAVE] 
            + а в случае смены состояния MM -- сбрасываем на CONFIRM_NONE

- По статусу GameResultsReady (106) соц.сервер должен забрать данные ROLL (результаты сессии), заперсистить изменения, 
и подтвердить сохранение результатов командой mm_close (после чего сессия, по идее, уйдет в 0)
    -[C++] запихать параметры SessionResults в JSON
    + соц.сервер принимает JSON_SessionResults (пока в обычном progress[] очередным, последним по счету, параметром)
        + можно сделать тест, который по нынешнему статусу "pvx сессия закончена" выбирает из статичного массивчика тестовый набор "SessionResults" и применяет
        - применяем SessionResults к пользовательской MD2 (и persist: save user data)
            + простые штуки (silver, rating..) идут прямо в соотв.переменную (пока что в db.xxx)
            - выдать новый талант -> в библиотеку
            -? выдать нового героя
            - поменять опыт/усталость указанного героя
        -!! изменения MD надо накопить в кармане, и отдать соц.клиенту по некому следующему запросу ("естьчо")
        + отправляем в сторону gateway команду "close"
    -[C++] на gateway команда mm_close превращается в обычный CancelMatchmaking (но с сообщением в лог)
    
+ mm_leave разрешаем также из 102
    + НЕТ, надо убрать этот вариант (Денис у себя убрал)

+ в состояниях 102, 104, 105/6, 107 автоматически поддерживаем сессию юзера на соц.сервере (пингуем, и по валидному onMmProgress делаем touch аккаунта)
- в состоянии 107 разрешаем login на соц.сервер (вероятно у юзера упал pvx клиент)
    - в остальных pvx-игровых состояниях (102, 104, 105/6) говорим "already logged in pvx"
    
+ формат апдейта mm-инфы в auth и set_fraction нужно поменять (на просто отдельные ключи "mm" и "mm_messages" в response), 
    + (а то там форменное безобразие с fake model change)	

! ВОПРОСЫ is_valid и апдейта MM на старте, при логине юзера
    -? при перезаходе игрока нужно обновить время создания сессии (create_ts), иначе is_valid:0 сессия сразу сбросится
    - mm_cancel разрешаем в состояниях 0, 1..100, 200 (когда игра стартовала, mm_cancel нельзя, только mm_leave)
        -? либо когда is_valid=0? -> впрочем, из is_valid=0 мы должны сами уметь прибивать серверную сессию
    - очень желательно подпереть всё так, чтобы при релогине stale mm-сессия оставалась в is_valid=0 (и оттуда уходила в progress=0, no active sessions) 
        ! БЕЗ травматичных сообщений на клиенте

------------
GATEWAY:
.-[C++] мапим команды "reconnect" и "leave" на соотв. методы RPC
.-[C++] команда mm_close превращается в обычный CancelMatchmaking (но с сообщением в лог)
.-[C++] запихать параметры SessionResults в JSON

------------
APPLY RESULTS:
.- изменения MD надо накопить в кармане, и отдать соц.клиенту по некому следующему запросу ("естьчо")
    .- выдать новый талант -> в библиотеку
        .- проверить, что такой талант существует
    -?? выдать нового героя 
        - проверить, что такой тип героя существует (и еще не разлочен?)
    .- поменять опыт указанного героя (по hero_crc32)
    .- поменять усталость указанного героя (по hero_crc32)

main.py:
    .- после завершения pvx сессии (шлем какую-нибудь команду?) получаем model changes с примененными session results
    
+ (bug:возвращается статус 104 (ingame) после отваливания pvx клиента из train-сессии (должно быть либо 107=dropped, либо 105/6=game finished))

-! НАПИСАТЬ НОВЫЙ mock_matchmaker, с отложенным апдейтом (сначала отвечаем из кэша, потом "якобы исполняем команды")
    - реализовать там все новые статусы (102, 104, 107, 106, 200 по таймауту accept)

=================================================================================
=================================================================================

THRIFT AGENT
+ отследить статус online (при служебном SubLogin ставить признак acc.sublogin, при нормальном заходе юзера - сбрасывать)
    ? на ThriftAgent завести локальный хэш логинов с timestamp-ами; если timestamp текущий, можно сразу ломиться на конкретный WS, без траты времени на SubLogin
    + также добавить на WS touch(acc), равно при сублогине и thrift-GM-запросах (чтобы работал хэш логинов на ThriftAgent)
    
- в процесс сублогина добавить обычное вытаскивание соц.инфы (может, вообще поменять LoginAction на sn_login или zz_login соответственно?) -> nick

- при старте thrift agent залезать на координатор за списком WS-серверов (и, наверное, регулярно апдейтить -- т.е. включить агентов в список для рассылки peer_ws апдейта; при первом же фейле, вероятно, выкидывать из списка?..)

! I32 currentsession поменять на что-нибудь внятное (STRING mmid хотя бы), сам интерфейс запроса вообще поменять на "вернуть сессию данного игрока" (по gameId к соц.серверу ломиться бессмысленно), с теми же параметрами (snid, snuid)
- состав сессии, видимо, придется вытаскивать из mm.debug_info
    ! для этого придется туда добавить соц.инфу остальных юзеров (чтобы не исполнять 10 лишних WS-запросов, по каждому юзеру)
    
- с героями, вроде, проблем нет: залезли в MD и вытащили/поменяли все что надо
    ! но если юзер при этом онлайн на сервере, нужно сохранить апдейты model change не в GM-запросе, а в "отложенной очереди response", 
    - и на следующую logic-команду вернуть юзеру "тут, кстати, естьчо" (!согласовать формат с Ваней/Вадимчиком)

=================================================================================
=================================================================================

+ восстановить main.py (включая матчмейкинг) в транке
    + и в бранче MD2
+ восстановить тесты на матчмейкинг (cluster.test.py, unit.test.py)

+ (fixed?) проблема с логинами (zzima? memcache/sql/mongo?)
    + добавить логгинга
    + поменять SafeLock на чистую thread.lock реализацию

NEW MODELDATA:
+ локальный генератор UID-ов для каждой ModelData (а не глобальный на уровне WS)
+ новые форматы ответов для matchmaking (убираем "fake MD changes")
+ (minor) избавимся от "replace" и "move" в протоколе model change update


=================================================================================
=================================================================================

NEW MODELDATA:
+ при создании ModelData в сервере нужно давать ей uidGenerator и changeHandler 
    // changeHandler по умолчанию будет ставиться в None; методы, работающие с ModelData, должны использовать декоратор @model_callback
+ решить проблему с уcтановкой нужного запроса (JsonHandler, SubAction) в качестве modeldata.changeHandler, когда у запроса вызывается callback 
    (с установкой changeHandler на onStart вполне может справиться кодогенерация) --> декоратор вешаем на callbacks, этот декоратор ставит changeHandler?..
+ новый класс CollectionKeyId(key:id) -> realdict(id:value)
    + поддержать обратное индексирование (id:key), тогда отпадет необх. в кросс-индексах
+ создание полной ModelData 
    + герои
    + таланты 
        + initHeroTalents
        + buildCrossIndexes
        + validateTalentSets
            + checkItemFit
                + validateTalentSets
                + BaseItemAction.checkItemFit
                - moveItem (для src и для dest)
    + инвентарь
        + initInventoryCommonTalents
+ перенос вещей (base class)
+ новые кастомные хэндлеры (SubAction-s)
    + move ts2ts
    + move ts2inv
    + move inv2ts
    + clear NewStatus
    + set act bar index
    + batch set act bar index	
- оживить тесты
    - unit.test
    - cluster.test
    + model.test
    + model2.test <-- сюда пока унес закомментированный код batch set action bar idx
+ выпилить лишние зависимости из SubAction
    + instr -> instr.py
    + touch -> helpers
    + сам SubAction + model_callback -> subaction.py
    + options. --> передавать снаружи?.. -> вынес отдельный xdbload.py
    + PW_VERSION_* -- используются в checkClientVersion, в разных вариантах login (zz_login, sn_login) --> вынес в базовый BaseCheckVersionAction
+ то же для xdb
    + убрать options.xdb_path?.. -> вынес отдельный xdbload.py
ТЕКУЩЕЕ
- НАГРУЗКА (main)
    - перевести нагрузочный механизм на fake facebook юзера (чтобы не требовалось 100500 zzima аккаунтов для нагр.тестирования)
    - ПРОВЕСТИ ПЕРВУЮ ИТЕРАЦИЮ ПО НАГРУЗКЕ (деплой + нагрузка хотя бы 1К CCU?)
- MODEL DATA, model changes (Ваня, Вадим)
    + ИНТЕГРИРОВАТЬ НОВУЮ MODELDATA, до уровня когда можно писать хэндлеры в новой логике
- для соц.клиента: поддержать принудительный gzip на стороне load balancer (unity/mono умеет задавать headers только для POST запросов)
- АП6 (срочные фиксы и хаки)
- аукцион (Костя Пикалов)
- thrift-логирование операций (Никита, Антон П.)
- запросы на чтение или изменение данных игрока от GM Tools (Антон П.)
    

=================================================================================
FUTURE
- FakeGameConnector для аукциона (~июнь)
- Fake GMTools "stat.request manager" (~июнь)
    - прячет от GMTools логику "админской сессии" (на каком сервере залоган нужный нам юзер, sublogin, смена серверов и т.п.)
        поскольку есть сессионная логика и понятие "пользовательского аккаунта" (== как минимум имя WS-сервера, куда нужно ломиться с запросами), 
        делаем потомком wserver, со своей логикой acc / modeldata? 
        // хотя нет, wserver сразу предполагает механизм login и залезания в persistent за user data..
- передавать код соц.сервера в Питер (~15-16 июня?)

=================================================================================

[!!! AP5 !!!] срочное по ModelData:
+ при загрузке из persistent:
    + добавлять новых героев (у кого не хватает)
    + убирать лищних героев (если их нет в XDB)
+ добавлять пустой actionBarId при операциях с вещами, чтобы восприятие {talent json} на стороне клиента C# стало однозначным

чек:
+ новые операции с биллингом, +тестик

С++:
+ при указанном ненулевом SessionLogin не позволять обычный тест-логин в pvx

+ кроме zz_login еще давать nickname (в матчмейкер)

ПО НАГРУЗКЕ:
============
    - Допиливаем SocialTestClient чтобы он поддерживал нагрузочное тестирование
    + Instrumentation: логгинг по отдельным операциям (actions): максимальное, среднее, суммарное время исполнения, кол-во операций.. и т.п.
        + накапливать это всё как базовую статистику IWebServer, периодически выводить в tick()
            // можно прямо в логике базовых класс-хэндлеров (на маппинге logged/unlogged/internal евентов) впилить общий инструментейшн
            + например, накапливаем кол-во вызванных handlers каждого типа (login, zz_login, auth, set_actbar_idx.. и т.п.), общее время исполнения (-> отсюда среднее)
                + хорошо бы отдельно накапливать данные по некоторому time frame (например "last 60 seconds") чтобы можно было мониторить

=================================================================================
+ описание пакетного set_action_bar_index
+ фикс пустого множества героев
+ фикс порядка добавления вещей в user inventory

NEAR FUTURE
===========
- Scribe (проконтролировать отправку thrift-логов)
    + прошерстить левые, чисто тестовые WARN
- подумать о поддержке операций game logic над несколькими юзер-аккаунтами с соотв. инстансами ModelData (передача вещей и т.п.)
    ? вероятные механизмы
        1) передача соседу read-only копии своей ModelData
        2) насильный релогин/миграция в сторону того сервера, где нужны оба персонажа
        3) peer-взаимодействие, с многоэтажным обменом сообщениями (это, конечно, FAIL по части прозрачности и простоты для "клиентской логики")
        4) выделение процессов "мульти-юзерной обработки" в отдельный processor server, который делает lock аккаунтов in question, и исполняет "админскую операцию"
- деплоймент на Amazon (PROD) и local linux (DEV)
- instrumentation и нагрузочное тестирование
    + Допиливаем SocialTestClient чтобы он поддерживал стартовые механизмы GameTestClient (подсовывание session keys через файлики)
    ! надо бы восстановить работу main.py на многих юзерах (что-то там с транспортом)
        + восстановил, очередь http-запросов там в обратном порядке разбиралась (как стек)
            теперь работает (пруфпик: http://gyazo.nivalnetwork.com/edit//082a0b1423e82ab5905e1b495b3cb86a.png)
    - логгинг по отдельным операциям (actions): максимальное, среднее, суммарное время исполнения, кол-во операций.. и т.п.
        - накапливать это всё как базовую статистику IWebServer, периодически выводить в tick()
            // можно прямо в логике базовых класс-хэндлеров (на маппинге logged/unlogged/internal евентов) впилить общий инструментейшн


!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
Grumo: persistent user[296352743483]
(подозрительный юзер, заходивший за разные фракции)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!


+ недочеты АП5: 
    + пакетная обработка action bar index!
+ админка: 
    + флаг для админ-юзеров (в sql-базе, видимо), некоторые команды могут проверять админский флаг на аккаунте
    + тестовый админский запрос -- снять CCU [peak CCU?] и количество юзеров в активном матчмейкинге (~progress 50)
+ рефакторинг: 
    + упрощение передачи интерфейсов (предусмотреть отдельный параметр I=iDict, из которого мапить всё?)
- матчмейкинг: более четкая стейт-машина (расписать в однозначном виде, учитывая и прогресс-статусы, присваиваемые соц.сервером)
    - 0=нет сессии
    - 1=соц.сервер принял mm_add
    - 2=gateway принял mm_add
    - 50=matchmaking принял (и поставил в очередь)
    - 100=смейкали, ждем accept
    - 101=соц.сервер принял accept
    - 102=gateway принял accept
    - 103=matchmaking принял accept, ждем остальных игроков
    - 104=все игроки зааксептили, выдан сессионный ключ
    - 105=мы пришли в pvx
    - 106=все игроки пришли в pvx, сессия стартовала
    ...
    - 199=сессия завершена, есть результаты (бонусы и т.п.)
    
    ошибки
    - 400=matchmaker не принял mm_add
    - 401=таймаут ожидания сессии (слишком долго ждали в статусе 50)
    - 402=таймаут ожидания accept (слишком долго ждали в статусе 100)
    ...
    ? 403=мы отвалилися из идущей сессии (можно реконнект)
    ? 404=сессия завершилась


=================================================================================

+ может быть, пора уже подумать над передачей всех интерфейсов пачкой, в виде iDict;
    + получим вызовы типа
        self.I.SQL.method(...), self.I.DATA.method(...)
        handler = NewHandler( I=self.I )
    + потребует, правда, очередного перелопачивания всех исходников и тестов
    
+ JsonHandler и подобные "обработчики многих событий" надо разнести по именам с thread-pool реализациями (ZZimaAsyncHandler) и "одно-евентовыми" extension Handlers
    + хэндлеры одного события(action=xxx) переименовываем в ***Action (или переименовать в Extension?) (LoginHandler, AuthHandler --> LoginAction, AuthAction)
    

=================================================================================
[Followup от 18.05.2011] 
2.	Производим рефакторинг кода социального сервера (Данила)
=================================================================================
-	a.	Разносим на Social Framework и специфичный для PW код
?	b.	Переносим код в Src/Server и в Game/PF
?/F c.	К 15 Июня настраиваем процесс периодической передачи Social Framework и других сервисов в Блицкриг
-	d.	Специфичный код по работе с Model Data пока остается в игровом коде, возможно мы его дооптимизируем и перенесем в Social Framework
=================================================================================

проблемы кросс-зависимостей ModelData:

!!! см. файл ".model.use_cases.txt"

    #~ - SubEvent.modelAddMatchmakingData
        #~ .\handlers.py:560: def modelAddMatchmakingData(self):		
        #~ .\ext\dataload.py:43: self.modelAddMatchmakingData()		## AuthHandler.		// "для нужд AP5 якобы засовываем matchmaking info внутрь ModelData"
        #~ .\ext\dataload.py:106: self.modelAddMatchmakingData()		## SetFractionHandler.

    #~ - SubEvent.validateMatchMakingSessions
        #~ .\handlers.py:562: self.validateMatchMakingSessions()  		## SubEvent.modelAddMatchmakingData
        #~ .\handlers.py:571: def validateMatchMakingSessions(self): 
        #~ .\ext\matchmake.py:135: self.validateMatchMakingSessions()	## MatchmakingPingHandler.
        
    #~ - model.buildCrossIndexes
        #~ .\model_data.py:270: def buildCrossIndexes(self, data):		
        #~ .\wserver.py:329: acc.model.buildCrossIndexes( acc.data )	## wserver.onUserData
        #~ .\ext\dataload.py:92: self.acc.model.buildCrossIndexes( self.acc.data )	## SetFractionHandler.
        
    #~ - WS.resetUserSessions
        #~ .\iwebserver.py:98: def resetUserSessions(self, acc):
        #~ .\pwserver.py:219: def resetUserSessions(self, acc):
        #~ .\wserver.py:318: ## self.resetUserSessions( acc )
        #~ .\ext\dataload.py:95: self.I.WS.resetUserSessions( self.acc )

=================================================================================

РЕФАКТОРИНГ под "FRAMEWORK"
+ рефакторинг wserver и coordinator под новый базовый интерфейс ITornadoServer (на котором написаны mock matchmaker/aggregator)
    - найти надежный под win/linux способ адресации поддиректорий ("ext" -> "../base" и т.п.)
    + login/logout, auth, zz_login, sn_login, аналогичные служебные хэндлеры -- оставляем во Framework
        + остальные ext/handlers сносим в Logic
        + делаем простой helper для загрузки целой поддиректории кастомных handlers (типа /ext/)
    + универсализировать апдейты coordinator->WS (сделать множество coord_cfg-опций настраиваемым?)
+ pw-шную специфику работы с ModelData, XDB -- убрать в отдельные интерфейсы
    + видимо, занести эту специфику под IModelData (по умолчанию там пустая модель и заглушка на месте _XDB)
    + IXDB, видимо, тоже понадобится (по умолчанию -- тупо None)
- ...
- PROFIT!

WSERVER
+ базовый рефакторинг
    + WorkServer
        + наследование, интерфейсы
        + init, tick, 
        + login/data-load pipeline
    + PwWorkServer
        + game-specific методы
+ cluster.test и unit.test сломались, они требуют self.WS 
    + с какой-то, причем, заглушечной начинкой as per старый конструктор WebServer() -- ну и даем им спокойно **kwargs, в виде PwWorkServer( MC=self.MC, ... )
+ matchmaking блок:: ФАКТИЧЕСКИ, ИЗОЛИРОВАННАЯ СУЩНОСТЬ? (WorkServerMatchmaker?)
    self.mmPacket, self.MM_PING_PERIOD
    self.I.PVX.handlePacket
    (ну и кросс-ссылки на методы блока)
    + ===> уйдет в game-specific

!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
+ onUserData блок: 
    + специфику ModelData уберем под интерфейс
    + стандартную работу с логинами и вытаскиванием данных из MC/DATA, вероятно, лучше вынести В ОТДЕЛЬНУЮ ПРОМЕЖУТОЧНУЮ СУЩНОСТЬ ("WorkServer", например)
        + WorkServer это сущность, которая работает с юзер-аккаунтами, поддерживает базовые механизмы login, loggedActions, IModelData 
          (в отличие от чисто служебных tornado-серверов, типа matchmaker или aggregator, где нет понятия "пользовательской сессии" и "юзер-аккаунта")


WHANDLER
+! updateClusterIpList: могут понадобиться оверрайды (набор конкретных опций coord_cfg может меняться в разных проектах)
    + пока не понял, куда пихать (вызывается из wserver и whandler, сам по себе @static и меняет только cfg; но может оверрайдиться в разных проектах)
    => наверное, это будет @static метод WS в итоге (аналогично ServerClass.initInterfaces)
!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
!видимо, придется полностью оверрайдить wserver.addHandlers (заменяя тип XxxHandler при маппинге logged/unlogged/internal событий)
+ есть блок для работы с play_fin (в specific) 
+ остальной блок -- стандартные команды от координатора (В ПРОМЕЖУТОЧНЫЙ КЛАСС WorkServerHandler?)


+COORDINATOR+
+ здесь все намного печальнее, методы сервера свалены в global namespace 
    + JsonHandler используется не общий, а свой (локальный copy-paste), onXxx-хэндлеры нужно занести в класс-потомок JsonHandler
    + define глобальные
    + для интерфейсов типа SQL тоже используются globals
    + блоки методов "check MC", "check SQL", "check/update peer WS" нужно занести в класс
+ ..Handler.onWebServerReady: предусмотреть оверрайд для списка опций, включаемых в апдейт 
    (вынести всякие ["mc_list"]=cfg.CLUSTER_MC_SERVER_LIST в отдельный метод)

=================================================================================

+ COORDINATOR_ZZIMA_API_* не апдейтятся с координатора на WS
+FUTURE админский флаг на аккаунтах

+ описание sn_login
http://confluence.nivalnetwork.com/pages/viewpage.action?pageId=135465054#%D0%9F%D1%80%D0%BE%D1%82%D0%BE%D0%BA%D0%BE%D0%BB%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%D1%81%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%BC%D0%BA%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%BC-3.1.1.3.%D0%9B%D0%BE%D0%B3%D0%B8%D0%BD%D1%87%D0%B5%D1%80%D0%B5%D0%B7%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%B9%D0%B0%D0%B3%D0%B3%D1%80%D0%B5%D0%B3%D0%B0%D1%82%D0%BE%D1%80%28%D0%B2%D0%B8%D0%B4%D0%B5%D0%B0%D0%BB%D0%B5%D0%BF%D0%BE%D0%B4%D0%B4%D0%B5%D1%80%D0%B6%D0%B8%D0%B2%D0%B0%D0%B5%D1%82%D0%B2%D1%81%D0%B5%D0%B2%D0%B8%D0%B4%D1%8B%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D1%85%D1%81%D0%B5%D1%82%D0%B5%D0%B9%29

=================================================================================

///////////////////////////////////////////////////////////////////////////////////////////////////////
// разбивка guid64 из нашей БД на server_id/auto_id
SELECT *, (id >> 32) as sid, (id & 0xFFFFFFFF) as zid FROM tbl_login t HAVING sid!=0 ORDER BY sid, zid;
///////////////////////////////////////////////////////////////////////////////////////////////////////

///////////////////////////////////////////////////////////////////////////////////////////////////////
// чтобы добавить новый интерфейс, прописываем его:
- для класса WebServer:
    1) в _Interfaces {}
    2) в application.add_handlers( XX=self.XX )
    3) в initInterfaces() iDict.XX 
- для класса RequestHandler:
    1) в свой _Interfaces {}
    (?) в _Interfaces базовых handlers: JsonHandler, SubEvent
///////////////////////////////////////////////////////////////////////////////////////////////////////


== FB ======================================
ID Приложения		185560451496273
Ключ API			a86aeecd736cd12b43891e33c1eca5b4
Секрет приложения	df7579f59caf37335d757c7569e4ffea

Данила
http://www.facebook.com/profile.php?id=100000234693443\

=================================================================================

https://www.facebook.com/dialog/oauth?client_id=185560451496273&redirect_uri=http://pw.nivalnetwork.com:8703/x
http://pw.nivalnetwork.com:8703/x?code=6bjj7EnvslCfgRR8eorvi9SV5u_RVUVXa-EYj3B0tVk.eyJpdiI6IndCdTBReTZiVnI0bmpVTkdsWnVqU0EifQ.Vr-vtfj5_mP9JkVeRHiOvxOS7MTMQZXNvU3Sjl78g0WwhnfbezD28bZiWNiXoD39fC7I2GoGhRZU66SecvpYvRTUz-5o2taiWRetrgmPDd3Dn52ABuh6OQJXQZ3vwm9sHSVTIrR0idjLNv-r-awEyw

https://www.facebook.com/dialog/oauth?client_id=185560451496273&redirect_uri=http://pw.nivalnetwork.com:8703/x&state=1234
http://pw.nivalnetwork.com:8703/x?code=MR5TcmFmr19W7w_bTasE1kqxkQUZjpZaeyzpoSOlOHo.eyJpdiI6IlZVaWZrdzRpR3hhbjF2cWI0Sk1mUEEifQ.RNvkgirTxj9p5Rjs1dtZ4BaVShEjiQZA14KogAV-CvBoReSfyyYL56yrgRWDI_Y_i1Cbi7VRt6lhy4u29FbMOK9aVADF62LqM8-Fs1M0XRmDXvMUA0b9PT0Su92txRJXGXJ_N693-NdyV-azf4Jkyw&state=1234

https://www.facebook.com/dialog/oauth?client_id=185560451496273&redirect_uri=http://pw.nivalnetwork.com:8703/x?action=login&state=1234
http://pw.nivalnetwork.com:8703/x?action=login&code=TdZSqeotrRlyuzmBPdpOuK4DYl_MHijozzgmOrMruwg.eyJpdiI6IjBtbkw2M190T3dwU2lTNTVEMXV5TXcifQ.oSqW8nvI4pZERTOPvTCwZqlThDyC-kPVhiro6E1P9rYEAjiz--LKNEiHZMUj7xvhxH-pxXR0wjqygoTXnBXiPLHbxRUuLiABlUPeOJkrZBXx87VwJi5cW1GKdUAbwptoeTPRm5m1KrsT-9ssvWOaTw&state=1234

https://www.facebook.com/dialog/oauth?client_id=185560451496273&redirect_uri=http://pw.nivalnetwork.com:8703/x?action=login&snid=fb
http://pw.nivalnetwork.com:8703/x?action=login&code=26bGR4V9Um1eo4Icc88SgjJJ9C1TYg4B-TKdj2qbQtQ.eyJpdiI6IjJxLXZHLW1MaVEya082SGdxZDQyTmcifQ.E5p1EImkmb98btgKYZ4k0T6oN8Azos3xPfolR_JD0kJruvuxdT4EtGEBzKmm8fQDb2zIPZIndFi4080gaLTpnYgXacl4d3QbEK_JQ7x14d_r57rJnvp1zbgCLbgJ8M9q20StJpFBTXl_PHHC_jV5bQ

https://www.facebook.com/dialog/oauth?client_id=185560451496273&redirect_uri=http://pw.nivalnetwork.com:8703/x?action=login&state=fb:user1234:rnd12345
http://pw.nivalnetwork.com:8703/x?action=login&code=aR0ayPcRdmUuVnJyEOE2WHmf_fuQWrSnfY3ADf3HtYA.eyJpdiI6Ik1wb2wwMmQ1dXZnTWNSNG5DMnQweHcifQ.klvv9qRBZZAX_9OrJCaXq4W60w9PNB0uCi89BTzpew1PUEJQm-091OLJH9IiHACK3P5I2whYoFAt7NJBNXbjyEmK93S54G7NCs57wRG8ajQpwjRUIHcy5zaksRGY6CaEI-7QCcDt7hUGp1SDemYWsg&state=fb:user1234:rnd12345

====

https://www.facebook.com/dialog/oauth?client_id=185560451496273&redirect_uri=http://localhost:88/one?action=login&response_type=token
http://localhost:88/one?action=login#access_token=185560451496273%7C2.J7LScesxZe9_7MIWv6Nn5A__.3600.1305633600.1-100000234693443%7CULdl12K61aJ4Lg7wOs20wRjzcc8&expires_in=5994
// однако получаем мы только "http://localhost:88/one?action=login" (URI fragment отрезается)

https://graph.facebook.com/me?access_token=185560451496273%7C2.J7LScesxZe9_7MIWv6Nn5A__.3600.1305633600.1-100000234693443%7CULdl12K61aJ4Lg7wOs20wRjzcc8
{
   "id": "100000234693443",
   "name": "\u0414\u0430\u043d\u0438\u043b\u0430 \u0421\u0435\u0440\u0433\u0435\u0435\u0432",
   "first_name": "\u0414\u0430\u043d\u0438\u043b\u0430",
   "last_name": "\u0421\u0435\u0440\u0433\u0435\u0435\u0432",
   "link": "http://www.facebook.com/profile.php?id=100000234693443",
   "gender": "male",
   "timezone": 4,
   "locale": "ru_RU",
   "verified": true,
   "updated_time": "2011-05-17T09:33:17+0000"
}


=================================================================================
[ROAD MAP] мой собственный, до собрания 18.05
=================================================================================
- АВТОРИЗАЦИЯ
    ? настоящий facebook login (как отдельный сценарий для mock aggregator)
    -? авторизация challenge/response (совершенно непонятно, как должна работать при фейсбучной sntoken-авторизации => sntoken, видимо, через https пойдет)
- РЕФАКТОРИНГ КЛАСТЕРНЫХ МЕХАНИЗМОВ
    + рефакторинг wserver и coordinator под новый базовый интерфейс ITornadoServer (на котором написаны mock matchmaker/aggregator)
    -! непонятно что происходит с mongoDB при выходе юзера и при shutdown рабочего сервера (MC и SQL обрабатываются, монго - что-то не факт)
    - автоматическая генерация nginx config, авто-раздача имен и портов WS-серверам (тогда запускаем WS-ки просто с адресом координатора, как и должно быть)
- ИГРОВАЯ ЛОГИКА
    -FUTURE настоящий HTTP SEND со статистикой и бонусами после окончания игровой сессии (из PvX в SS)
    -FUTURE валидация талантов: рассылка массивной, но все же подъемной выжимки "дерева талантов" с координатора по всем WS
    -FUTURE дополнительные команды для интерфейса герои/таланты (создать героя, удалить героя, создать вещь, удалить вещь)
    ?-FUTURE домики (вероятно, лобби-тим будет делать, после Framework-рефакторинга)
=================================================================================

SN_LOGIN
+ отдельный action=sn_login с обращением по SOCIAL_AGGREGATOR_ADDRESS
+ сделать тест 
    ? какие, вообще, могут быть НЕвалидные логины? кроме откровенно левых (типа отсутствия необх. параметров)
+ нужен ли отдельный ISocialAggregator (хм, для тестов, наверное, нужен..)
    + подумать над интерфейсом для скидывания на JsonHandler тестовых событий, наподобие SubEvent 
        (в норме JsonHandler наследуется от tornado.web.RequestHandler, и норовит отправлять response по http)
        +++ прекрасно достигается двойным наследованием: (SubEvent, JsonHandler), при этом fin/__init__ SubHandler'а перекрывают http-специфику JsonHandler


MOCK_AGGREGATOR
http://confluence.nivalnetwork.com/display/prog/Social+Aggregator+Reqs+%28Iteration+1%29
+ скопировать оболочку (архитектура как у mock_matchmaker)
+ специфичная обработка запросов (слегка другой формат response/error)
+ тестовые запросы к mock aggregator: login, register, get_info
+ в конфиге добавить настройки aggregator (как минимум адрес для http-запросов)


=================================================================================
+ не проходит cluster test (надо найти common talent с дубликатным ID)

+ нужно высылать список ActionBarIdx для талантов: на gateway и дальше в matchmaking

ВАЛИДАЦИЯ ТАЛАНТОВ
[NOW]
+ валидация списка талантов после OnUserData 
+ скрипт для экспорта XDB/XML-данных в svn

+ для конфы:
        "ts_xdb_time":<date_time_string>, // (!) v.124: время последнего апдейта XDB/XML-талантов на социальном сервере
                                          // (svn update талантов + генерация pck-файла для сервера)
[FUTURE]
? рассылка массивной, но все же подъемной выжимки "дерева талантов" с координатора по всем WS
    ? валидация: пробежаться по всем активным юзерам, после получения coord_talent_tree_update
?? проверка синхронности версий "дерева талантов" на соц.сервере и соц.клиенте (скажем, заставить считать checksum от JSON-дампа дерева талантов определенного вида)
    + тут пока можно обойтись timestamp ("ts_xdb_time") как примерным индикатором "актуальности дерева талантов"


=================================================================================
CONFIG
+ вынести pvx_login_address (и zzima-параметры, кстати) в конфиг координатора (а то запаримся править уже на нескольких отдельных WS)
    CLUSTER_PVX_MATCHMAKER
    PVX_LOGIN_ADDRESS
+ ZZIMA: 'fromServiceName' : 'pwtst', #TODO: надо брать из cfg
    ZZIMA_WSDL

DEPLOY
+ дописать Pvx Deployment ("gateway")
+ закоммитить новые rename.cmd/run.cmd
+ правильное имя лог-файла для gateway
? заняться спавнами и pid для соц.сервер [ре]стартера

-! непонятно что происходит с mongoDB при выходе юзера и при shutdown рабочего сервера (MC и SQL обрабатываются, монго - что-то не факт)


=================================================================================
+ тезисы для разбора полётов

- HTTP SEND со статистикой после окончания игровой сессии (из PvX в SS)
    o	reliability +доигравшим до конца сессии, -leavers
    o	+rating (winners)
    o	+10 zzgold (winners)
    o	+случайный талант (winners)

+ возможность узнавать на клиенте сколько за какую сторону матчмейкается -> чуть допилить возврат прогресса сессии, +параметры к списку [ progress, session_id {, ...} ]
    => сделано в виде строки debug_info (через запятую логины, статусы и т.п.)

=================================================================================
+ (Денис) в клиенте я ожидаю строку session_id вида localhost:35001/login/e3921387ae0f5dfe204e05d1e7bc51be
    + login_address пока в .cfg

MATCHMAKING (next)
+ победить проблемы с social/C++ matchmaking
? возвращаемое значение из Login:: AddSessionKey

+ (1) валидация mm-сессии на соц.сервере (например по lastPingSuccess) -> ставим is_valid = 0|1
    + особенно после загрузки из persistent
    
+ разрешить _.@ в логинах

+ передача адреса логина с сервера на клиент -> +параметр к <session_id> = "login/session_key"?
+ улучшение ситуации с логами на клиенте
? возможность менять сторону на клиенте - интересно как это мы свитчимся к красным, если у нас домики за синих?

=================================================================================
ITEMS (next)
+ новое property у талантов: "ActionBarIdx" (серверу, имхо, необязательно даже проверять, пусть клиент старается; 0=None, можно не сохранять)
+ проверка пререквизитов талантов 
    + разбор новой XML с наследованием 
    + сама проверка
    + unit test (убираем апгрейд-таланты и их пререк, пробуем вставить апгрейд-талант без пререка->fail, вставляем пререк, пробуем вставить апгрейд-талант->ок)
    
    
X -- ДИЗАЙН ОБНОВИЛСЯ, больше не требуется 
X чтобы решить проблему с "убиранием из TS таланта, который является единственным пререком к другим талантам в TS"
    + построить индекс prereq_talent_id -> [ follower_talent_ids ]
    + построить кросс-индекс (от ModelData): acc.data.index.HeroTalents{ hero_id->talent_id->slot }
    X проверять по кросс-индексу ситуацию "есть талант-потомок данного таланта, у которого нет других пререков, кроме данного таланта", 
        не давать выкидывать талант, если в TalentSet есть завязанные только на него "апгрейды"

=================================================================================
SESSION LOGIN (2)
+ починить 64-битные гуиды
    + matchmaking и SessionLogin теперь получают только zz_uid_32
+ сделать новый механизм прокидывания session_id (добавить в ответ на mm_ping, a не на mm_accept)
    + ping теперь возвращает [progress, session_id=0]
    
+ добавляем zz_login, zz_uid в matchmaking.mm_add
    + при обычном action=login заполняем zz_login/zz_uid заведомой лабудой (= social login, social uid), надеюсь, ZZima не развалится от левых реквизитов
+ добавляем user_id, zz_login, zz_uid в Login::AddSessionKey
+ оставляем только одну опцию "session_login" для старта логин-сервиса
X проносим опцию "session_login" внутрь ZZima worker thread; в случае sessionLogin zzima-авторизацию не проводим (оставляем userId с уровня SessionLogin)
    + как вариант: опцию можно не проносить, признаком SessionLogin считаем nUserID==0 в context.resultData
+ на уровне SessionLogin вытаскиваем, кроме sessionPath, и остальное, что пришло из matchmaking gateway (?user_id?, zz_login, zz_uid), 
    + сразу пихаем nUserID в context.resultData


=================================================================================
URGENT
+ поправить работу с faction/fraction, "N|A|B" / 0|1|2
    + ок, буду передавать "fraction":0|1|2 (0=любая, 1=A, 2=B)
+ добавить update MatchMaker -> SessionLogin	
    + gatekeeper Login::clusterServerId
    + LoginServerAsync::AddSessionKey( sessionKey, sessionPath )	

    
=================================================================================
SESSION LOGIN
+ на LoginServerAsync заводим map<sessionKey -> [userId?], sessionPath, [md5_pwd для проверки подписи?]> 
+ добавляем REMOTE методы 
    + LoginServerAsync::AddSessionKey( sessionKey, sessionPath ), 
    + LoginServerAsync::RemoveSessionKey( [userId?], sessionKey )
+ добавляем новое поле sessionKey в LoginRequest (pvx клиент узнает sessionKey от социального клиента)
+ новое поле sessionPath в LoginResult (куда ломиться клиенту после логина, чтобы попасть в свою игру)
+ пара новых опций для логина ("разрешать логин через sessionKey" и "запрещать логин НЕ через sessionKey (т.е. запрещать старый логин, через login/pwd)")
    + add_service_option session_login login
    + add_service_option session_login_only login

=================================================================================
TALENT SET!
+ передавать TalentSet в игровую сессию (при mm_add или mm_accept)
"talent_set": { 
    <slot>: { // <slot>: номер слота=1..36
        "id":<talent_id_crc32>, // например, crc32("AssassinA0")
        "rr":<refine_rate> 		// <refine_rate>: уровень заточки (сейчас всегда=0)
    }, 
}
+ в HttpGateway уметь парсить из этого JSON-словаря вектор записей вида { slot, talent_id, refine_rate }
    + vector<STalentSetItem> talentSet;

=================================================================================
+ новый механизм проверки, что талант подходит в слот TalentSet героя (через отдельную экспортную Talents.XML)
    + надо, понятно, научиться разбирать эту Talents.XML

=================================================================================
ZZIMA LOGIN для соц.клиента
+ оценить стоимость реализации 
    X 1) через C++ кластер (питон -> HttpGateway -> ...RPC?... -> ZZima login processor)
        X добавляем в LoginServerAsync аналог ProcessNewContext( context ), при этом у context заводим признак "левый (gateway) логин"
        X добавляем REMOTE LoginServerAsync::GatewayLogin( login, pwd ), по завершению обработки -- вместо SendReply отправляем ответ для HttpGateway
    + 2) через прямые HTTP-вызовы из питона
        + простой и прозрачный примерчик в soap_example.py
        + если соц.серверу удается залогиниться в zzima, то автоматом создаем нового persistent юзера 
            X с заполненной колонкой zzima_login / zzima_md5
            + новому юзеру даем имя с префиксом = "zzima_<zz_login>", зайти им не через action=zz_login не получится (в action=login подчеркивание - недопустимый символ)
        X придется, однако, предусмотреть и специальный режим проверки по memcache (может, однозначные префиксы придумать для разных соц.сетей?)
            X причем операции именно с MC надо будет вести по "мета-логину", а вот с .DATA надо общаться по однозначному "нашему" persistent login
    X кстати, получается 2 запроса к разным БД-шардам: 
        1) server_login = sp_find_login( zzima_login ), 
        2) sp_login( server_login )
        при попытке логина через MC -- пробуем по zzima_login, а в формат memcache-записи ("pwd:server:..."), видимо, добавляем server_login и sn_pwd
        

=================================================================================
+ session_id вынести из FIN{} наружу (на top-уровень параметров сессии)
+ в ответ на cancel присылать $del mmid (cancel ALL -> del весь MatchList)
+ митинг (1ч), распечатать схему соц.кластера
-FUTURE для юнит-тестов завести отдельный "дефолтный hero_crc32" (напр. -1), чтобы не грузить XDB-кирпичи

+ поддержать новый порядок генерации hero_crc32 и ability_crc32
+ поддержать fraction=A|B|N, hero=<hero_crc32>
+ поддержать передачу matchmaking-данных внутри ModelData для auth и set_fraction
    + на самом деле передаются как отдельные _изменения_ в ModelData ( тройки внутри model:[] )
+ возвращать некий (произвольный) session_id в ответ на accept
    + например, просто поставить генератор ключа "sk_<timestamp>_<auto_inc>" внутри mock_matchmaker

=================================================================================
+ guest login
    X можно поддержать авто-сохранение нового логина в БД, при некотором указанном ключе в action login ("guest=1" ?)
        X т.е. на этапе "такого юзера нет в БД" не обламываемся, а нордично сохраняем новенькую пару login/pwd 
            (надо только придумать как быть с непересечением user_id в разных базах, что для userNNN делалось либо ручками, либо насильной записью id=NNN)
            (через координатор распределять четность -- будут проблемы с синхронизацией; мастер-БД и мастер-таблица?.. memcache lock->опять async)
    + другой вариант - вообще не сохранять ничего в persistent; 
        выдаем временный uid, создаем разные fake-ключи про то что "авторизация пройдена, все данные загружены, и вообще у нас успешный релогин"
    
-FUTURE дополнительные команды для интерфейса герои/таланты
    - создать героя: class, level, id_type=с каким hero_id создавать (crc32 или autoinc)
        >> ok, model hero_id:value
    - удалить героя: hero_id, [контрольные: class, level]
        >> ok, model $del
    - создать вещь (талант) в инвентаре: talent_id(=crc32), [контрольные: href item name?]; item_id выдаем по autoinc
        >> ok, model ItemsKeeper/ item_id:{}, Inventory/ $add
    - удалить вещь (талант) из инвентаря: item_id, [контрольные: TalentId]
        >> ok, model ItemsKeeper/ $del, Inventory/ $del
    -OPTIONAL удалить вещь (талант) из списка талантов: slot, item_id, [контрольные: TalentId]
        >> ok, model ItemsKeeper/ $del, Inventory/ $del


=================================================================================
MATCHMAKING
+ в случае таймаута на accept, матчмейкинг вполне может прислать снова статус "сессия в процессе матчмейкинга", надо быть готовым удалить ключи типа FIN:{accept:}
+ прогресс сессии может включать данные о "количестве accept-ов" (например, 1-99=in progress, 100=done, 200=accepted by me, 101..109/201..209=accepted by N others)

[1.25 + 0.5]
+ после реализации протокола "AddRequest/Accept/.." на стороне matchmaking -- заменить запросы к заглушке на нормальные RPC-запросы
- сделать эмуляцию отправки "результатов сессии" в C++ (HttpJson) -- видимо, результаты будут скидываться на внутри-кластерный listener сервиса "gateway"
+ [Денис] SessionLogin пока нафиг убираем, эмулировать session login будет прямо лобби 


=================================================================================
С/S CURRENT
+ допилить еще 2 варианта item_move 
    + ts2inv 
    + inv2ts
+ отразить изменения в ModelData по FUP Антона от 05.04
    + перейти на новый Inventory, с простым списком [itemID, itemID, ...] 
        >> json encode делает автоматическое преобразование dict->list, и добавляет в конце списка маркер "$dict"
    + добавить талантам property "PositionID" 

+ добавить проверки, что данный итем (талант) подходит данному герою в данный слот (ряд слотов) TalentSet
    + проверяем, что талант вообще есть у нашего героя
    + проверяем, что уровень таланта (у нашего героя?) подходит к уровню слота, строчки по 6 талантов (1..6) .. (31..36)
    +FUTURE: текущая проверка, что талант есть у героя в default TalentSet, вообще-то некорректная (потом полезут всякие левые таланты, заточки и т.п.)
        но на данный момент это то что есть
+ выкатить версию на амазон
    + заодно добавил батник на копирование выборки XDB-шек в tornado.src/xdb

=================================================================================
CLIENT/SOC.SERVER:
+ попробовать прикрутить gzip на persistence и клиент/сервер (ужать 50К ModelData)
    - для С++ gateway нужно уметь разбирать gzip-ответ 

+ проблема item_move в строчных ключах (почему-то model содержит именно строчные ключи слотов, вместо числовых) 
    + недоработка parse при десериализации, вероятно)

+ новые команды [7.5 + 0.5 xdb]
    + item_move (generic, с полными путями) http://confluence.nivalnetwork.com/pages/viewpage.action?pageId=135465054#%D0%9F%D1%80%D0%BE%D1%82%D0%BE%D0%BA%D0%BE%D0%BB%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%D1%81%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%BC%D0%BA%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%BC-itemmove
    + item_remove_new_status 	http://confluence.nivalnetwork.com/pages/viewpage.action?pageId=135465054#%D0%9F%D1%80%D0%BE%D1%82%D0%BE%D0%BA%D0%BE%D0%BB%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%D1%81%D1%81%D0%BE%D1%86%D0%B8%D0%B0%D0%BB%D1%8C%D0%BD%D1%8B%D0%BC%D0%BA%D0%BB%D0%B8%D0%B5%D0%BD%D1%82%D0%BE%D0%BC-itemremovenewstatus
    
+ item_move делим на 3 команды (ЗАЧЕМ? "так нада".)
    + move_ts2ts <src_hero_id> <src_slot_id> <src_item_id> <dest_slot_id> <dest_item_id>
    + move_ts2inv <src_hero_id> <src_ts_slot> <src_item_id>
    + move_inv2ts <src_item_id> <dest_hero_id> <dest_slot_id> <dest_item_id>
    
+ определиться с изменением форматов auth и set_fraction
    >> переходим на [op, path, json_changes], при этом <path>="" (пустой == корень ModelData)
+ определиться с новым форматом mm-команд (ModelData)
    >> <path>="PvPMatchmaking/MatchList/...ключи сессий как у тебя было"
-BUG: неправильный формат ответа mongoDB (нет ключа "r"[0], характерного для mysql SELECT-ов)
+BUG: когда отсылаем mm_add "вне очереди", не запрашивая при этом периодический mm_ping всех сессий, отсутствие в pvx-ответе прогресса текущих сессий прибивает их!


.103
+ прикрутить глобальные индексы героев/талантов (CRC32 -> name)
+ уметь создавать структуру ModelData в питоне
    + вопросы: как правильно раздавать slotID, heroID, itemID
        +FUTURE: перевести heroID с CRC(DBID) на выдачу нормального порядового heroId на юзере (как вариант сделал, закомментил)
    -X- создавать пустой новый итем (тоже произвольная организация)
+ операция "добавить героя" вероятно нужна: даем имя класса героя (или crc32), сервер добавляет героя с дефолтным для этого класса набором талантов
    + создавать пустого нового героя (хотя бы с properties и произвольным перечислением талантов)
        + пока что либо создаем случайных, либо полный набор (heroId == crc32 соотв. героя)
    + корректно наполнять список абилок + его индексы (slots)
-X- то же самое для вещей: 
    -X- откуда они у героя возьмутся-то?
    -X- где вообще описаны типы вещей (XDB?) -> НИГДЕ, их еще нет


.102
+ параметр key покастовать к STRING (сейчас сервер отдает после логина 12345 числом)
+ новая команда set_fraction (заодно позволяет сохранять с клиента ModelData)


=================================================================================
HTTP GATEWAY:
+ заводим JSON-объект (root) с иерархией сессий/пингов (по всем WS)
+ приходящие запросы (add, ping, cancel, accept) отправляем в сторону matchmaking; ответы (callbacks для add/ping) записываем в root-иерархию
    - после реализации на удаленной стороне -- заменить запросы к заглушке на RPC-запросы
+ мгновенные http-ответы на ping черпаем из текущей root-иерархии (нету соотв. ключа -- значит, на этот раз не включаем в ответ)
+ как быть с ответами на add? юзер же хочет получить id сессии, наверно? 
    >> WS все равно выдает свой mm_ идентификатор сессии, мы просто мапим mm_XXX_YY на requestId
+ сделать тест для ParseJson (разбора команд и ответа на них)

PVX HTTP SEND:
+ интегрировать cURLpp (? или просто libcurl, или какую-нибудь обертку поудобней)
+ сделать эмуляцию отправки "результатов сессии" в python (mock_matchmaker)

DATA MODEL:
+ сохранение тестовых "данных юзера"
    action=set_fraction, fraction=A|B|N, model=<json data>
+ статические "табличные данные", импортируемые из XDB: 
    + список героев
        * из /Data/Heroes/_.HRDB.xdb берем HeroesDB/Heroes/Item***/hero href=<hero file link> 
        -> id героя := crc32(<hero file link>)
    + список талантов и т.п.
        * из _.HRDB.xdb берем тот же самый .HeroesDB/Heroes/Item***/hero href=<hero file link>
        * из <hero file link> достаем .Hero/defaultTalentsSet href=<talentset file link>
        * из <talentset file link> достаем .TalentsSet/levels/Item***/talents/Item***/talent href=<talent file link> 
        -> id таланта := crc32(<talent file link>) 
            
    // загруженные таблички талантов можем потом использовать как default при создании "persistent данных нового юзера"

=================================================================================
[FollowUp с Антоном К]
=================================================================================
- обсудили существующие модели данных.
- рассмотрели текущую реализацию героев инвенторя и талантов в ModelData lobby client, принципиально она НЕ конфликтует с серверным функционалом.
- договорились:
1) на стороне Lobby Data Model максимально привести структуры данных к виду { id:value }, особенно там, где и так используются уникальные id.
2) на стороне сервера максимально поддерживать существующую Model Data, в виде "дерева persistent данных юзера".
3) формат описания Data Model для клиент/серверного взаимодействия привести к виду существующих документов по JSON-форматам на confluence
(напр. http://confluence.nivalnetwork.com/pages/viewpage.action?pageId=135465048#%D0%9F%D1%80%D0%BE%D1%82%D0%BE%D0%BA%D0%BE%D0%BB%D0%BE%D0%B1%D1%89%D0%B5%D0%BD%D0%B8%D1%8F%D1%81PvXmatchmaking-pvxping)

порядок работ:
1) на стороне лобби-клиента делается логин и авторизация, handshake с текущим бегающим соц.сервером (Антон)
2) на стороне соц.сервера делаются механизмы для приема инициализационных данных (Данила)
    - сохранение тестовых "данных юзера", 
    - сохранение статических "табличных данных", импортируемых из XDB: список героев, список талантов и т.п.
3) описание JSON Data Model лобби выкладывается на confluence (Антон)
4) дальше описываем и начинаем реализовывать первый набор операций с данными игрока (heroes, inventory, buildings.. что-то из этого)

=================================================================================
[FollowUp с Женей]
=================================================================================
1) персональные данные игрока (TalentSet, Heroes, Inventory; ping stats, rating, reliability и т.п.) должны входить в состав persistent data, 
    и уходить параметрами при создании mm-сессии.
    - пинги: до разных игровых кластеров (US_WEST, US_EAST, EUROPE, CHINA и т.п.)
2) после завершения игровой сессии результаты должны приходить социальному серверу (и опять-таки агрегироваться в user persistent data)

порядок работ:
- обсуждение данных и протоколов
- диаграммы активности 
- описание протокола
- реализация


=================================================================================
[FollowUp про связь Social Server <-> PvX Matchmaking]
=================================================================================
- Нарисовать схему работы PvX клиента после запуска его из социального сервера, свести с существующей диаграммой активности старта сессии (Денис, ДАНИЛА)
- Вырабатываем необходимые изменения протокола общения и отражаем их на конфе (Данила)

+? Ищем HTTP C++ библиотеку для связи с Social Server (Данила) -?- пока взял Lacewing, но она без исходников (потом заменим на родной ACE http сервер)
+ Ищем JSON C++ библиотеку для связи с Social Server (Данила) -- jsonCpp, адекватна.
+? Даем возможность Денису писать С++ код по связи с социальным сервером – пишем удобную обертку для этого (Данила) -- примерчик есть
    +? можно сделать полную заглушку на C++, аналогичную mock_matchmaker.py -- ? нужно ли? - пока добавил пример разбора иерархии запроса "ping"

*   (ИГОРЬ, Данила, Денис) Расписываем схему по нотификации частей кластера о новой сессии и пользователе 
•	(Денис) Выносим matchmaking в отдельный сервис из lobby и приводим его к интерфейсу от Игоря 
•	(Денис) Делаем реализацию matchmaking с поддержкой рейтинга и reliability 
•	(Денис) Подключение к чату нужно поддержать на этапе загрузки 
•	(Денис) В lobby оставляем только custom game, рефакторим клиент и сервер 
•	(client и сервер) Оставляем возможность клиента в режиме запуска custom game 

=================================================================================
[FollowUp по Social Server Deployment]
=================================================================================
- Для социального сервера делаем ту же схему Trunk, Test, Production и поднимаем все пока два экземпляра (Trunk – pw.nivalnetwork.com и Prodiction – Amazon) 
- Код различных частей проекта храним в различных svn репозиториях, 
    + делаем проверки версий между парами участников взаимодействия (Данила, Антон)
+ Необходимо помнить о том что нам нужен high load тест на matchmaking, для чего нужен fake login (Данила) 
- Для социального сервера должна работать схема по настройке работающая по принципу “конфиги вручную правятся в репозитории” (Данила)

•	(Женя) Исследуем различные существующие средства развертывания (RAdmin) – одно из основных условий это работа на Unix и Windows операционках)
•	(Женя) Данные по серверам отдельно анализируем, принимаем решение по тому нужен ли Team City target для этого. 
           Также необходимо понять как будет организована работа дизайнеров с редактором и тд )

=================================================================================

[MongoDB]
+ сделать реализацию AsyncMongo для замены IData
    + порефакторить cluster_*, выделить общего предка AsyncClusterQueue, и от него уже вывести AsyncMongo (включая "современные технологии работы с queue" :)
+ persistent data: на уровне mongoDB это может быть find_and_update(): забирая данные, заодно стампим их своим server_id (чтобы никто больше не мог их успешно апдейтить)

[FUTURE] 
+ persistent data: неплохо бы поддержать mock-data, аналогично mock-login (дает юзеру случайную валидную иерархию данных, без всяких БД)
    + в т.ч. нынешнее общение с MC по поводу cached persistent data нужно убрать под виртуальные акцессоры вместо всяких там CasS/AppendS (чтобы можно было делать mock)
- доделать авторизацию "challenge-response" (после логина)
    + заодно в ответ на response отдавать клиенту (вероятно, уже подтянутые из storage) persistent-данные

=================================================================================

[Eugene] comment - 10/Mar/11 15:18
+ Написать доку по запуску тестового сценария тестовый клиент-social matchmaking support-matchmaking сервер
+ раздедить unit тесты и функциональные тесты
+ подумать на тему запуска кластера без sql c эмуляцией заполненного memcache
+ переназвать нагрузочных клиентов (сейчас test_), например async.load.py
+ WebServer -> SocialServer 
+ обсудить структуру документов на confluence и положить туда
+ переделать декларацию формата

=================================================================================

[к митингу]
* дока по протоколам
    http://devjira.nivalnetwork.com/browse/PF-25342
* картинки
    http://devjira.nivalnetwork.com/secure/attachment/37685/Social_Login.jpg
    http://devjira.nivalnetwork.com/secure/attachment/37997/Matchmaking+use-case+for+soc.client.jpg
    
[осталось по Use-case]
+ в конфиг координатора надо добавить адрес pvx-сервера (http), апдейтить его WS-кам
+ обработать ответы pvx.cancel/pvx.accept (есть ли сценарии, где это важно?)
    + как минимум, можно сделать механизм доставки сообщений "mm_error" юзеру (при запросе очередного пинга)
+ добавил команду logout (WAS: тестовое сохранение после удачного завершения matchmaking?)
+ (WAS: сделать заглушку IMatchmaker внешней, а не внутренней (tornado-based, и слушать http?))
    + лучше сделать нормальный вариант IHttpMatchmaker, 
        + а интерфейс заглушки привести к "симуляции async http callback"
        
+ написать тесты matchmaking (включая сценарий для тестового клиентика?) 
+ все-таки сделать внешний mock-сервис, поддерживающий IMatchmaker (tornado-based server)

+- документация на 2 интерфейса 
    + клиент/сервер 
    +- сервер/pvx-mm
+ нарисовать схемку (соц.клиент - соц.сервер - pvx mm)

=================================================================================
[ хэндлеры для обмена с клиентом ]
+ add mm
+ ping mm [ALL] status
+ accept/cancel mm
    + cancel отменяет add/ping с аналогичным uid+mmid
    + ping/cancel (если там не 0) проверяют наличие такого mmid
    + accept проверяет наличие завершенной мм-сессии (скажем, атрибут "FIN" у записи)
    ?- у matchmaking сессии, вероятно, есть timeout (и/или периодичность запроса PvX ping)
+ при загрузке данных юзера (из MC) – добавлять запрос к PvX-части, на апдейт его старых mmid (скорее всего, уже протухли, но мало ли)
    >> это может входить в механизм "периодического пинга": добавляем к каждому mm[uid] и/или [mmid] поля 
        + tReqd=timestamp последнего разпроса пинга; // для записей полученных из MC tReqd принудительно сбрасываем в 0 (force ping в следующем пакете)
        ? tRecv=последний ответ PvX;
        
=================================================================================
[ запрос SS -> PvX ]
+ формирование запроса (WS)
+ разбор запроса (PvX-заглушка)
    mm: {
      ping|cancel: {
        uid: [mmid, ...] | 0=all,
      },
      add: {
        uid: { 
          mm_id: { session_type, fraction, hero_type },
        }
      },
      accept: {
        uid: mm_id,
      }, 
    }		

[ ответ PvX -> SS ]
+ формирование ответа (PvX-заглушка)
+ разбор ответа (WS)
    + sessions (progress, accepted-статус и т.п.)
    ? cancel 
    ? accept

    mm_reply: {
        sessions: { # одинаковая реакция на ping и add (при успехе uid.mmid.progress>0)
            uid: {
                mmid: progress>0 (1=started, 100=matchmake done, session ready) | 0=fail (no such),
                ...
            }
        },
        
        cancel|accept: { # отдельные разделы для подтверждений accept/cancel
            uid: { mmid:0|1 (success), .. } или 0|1 (success ALL)
        },
    
    ???	notify: [ # здесь PvX может передать какие-то сообщения (warnings?) для соц.сервера; напр. насчет протухания сессий?
        { message, ... },
        ]
    }

=================================================================================
[ Новый use-case: общение с matchmaking ] -> наверно, в отдельный root

+ подумать над персистом данных; 1) persist-данные должны быть надежны на уровне логин-локов; 2) динамическая инфа об идущих у юзера матчмейках не должна теряться 
    + как минимум, при релогинах (>> MC): туда автоматом выкладывается весь acc.data (где и хранятся юзерские данные .mm)
?-- на уровне фейла компонентов - проще на всяк.случай уточнять у PvX matchmaking, нет ли других (неизвестных текущему WS) mm-сессий данного юзера

+ хэндлеры для обмена с клиентом 
    + add mm, 
    + ping mm status
    + accept/cancel mm
+ сборка пакетов для обмена с PvX Matchmaking, send/response (большой пакет, с набором команд по индивидуальным mmid)
    ? в т.ч. некоторые правила для убирания предыдущих команд из текущей сборки (напр. когда подряд create mmid_xxx -> cancel mmid_xxx)

+ тестовая dummy-заглушка, эмулирующая сторону PvX Matchmaking (create/cancel чего скажут; процесс матчмейкинга == просто таймер; в конце случайно выбираем N mmid-шек)
    + нарисовать схемку общения (соц.клиент | соц.сервер | pvx сервер)

+ для поддержки "глобально-уникальной нумерации" (вещей, mm-сессий и т.п.) нужно ужесточить работу координатора и WS с server_id 
    (int64; при старте брать не 1, а следующий AUTO_INC из БД)

=================================================================================
[[PF-25202 Persistence данных matchmaking ]]
+ persistent данные юзера: если храним в нереляционной БД, нужно предусмотреть дополнительный lock данных
(т.к. в нереляционной части аггрегация разных таблиц, типа tbl_servers и tbl_login, не получится.)  
+ например, сервер, впервые грузящий данные юзера, получает некоторый дополнительный STAMP, который должен соответствовать в nosql.data { };
    update впоследствии допускается только по соответствию и data_key, и STAMP (может, кстати, совпадать с server_id)
    -- на уровне mongoDB это может быть find_and_update(): забирая данные, заодно стампим их своим server_id (чтобы никто больше не мог их успешно апдейтить)
    + причем в случае ws_skip, опять же, знаем, чей server_id нам надо оверрайдить (и сервер, у которого мы "хайджекнули" юзера, уже не сможет его тронуть)
+ в это случае logout юзера надо выполнять только ПОСЛЕ того как убедимся, что его данные слиты в data-хранилище 
    >> юзер на это время может получать статус "is being logged out", там надо не забыть logout-attempt (при живом WS-хозяине никто не сможет овверрайдить логин-лок)

=================================================================================

[ Поддержка мягкого рестарта сервисов ]
+ при рестарте с изменившимся набором БД – текущие операции с SQL-интерфейсом (старым набором БД) должны завершиться, и аккуратно свалить, подменившись новым интерфейсом
+++ написать тестик на смену SQL-конфига "на лету"
+ рабочие сервера тоже должны уметь делать "мягкий shutdown": завершать все текущие запросы и сваливать после Ctrl+C или "kill -s SIGINT <pid>"
+ сделать "instrumentation времени ответа" отключаемым/заменяемым механизмом (чтобы не загромождать ответы shipping многоэтажными таймстэмпами)

[ Доработка кода для оптимизации работы с EC2 ]
+ радикально уменьшить логгинг
+ добавить метрики (время исполнения операций и т.п.) к http-ответам клиенту
+ реализовать для thread-pools вариант с wait(condition) вместо sleep, по идее очень хорошо подходит именно к варианту "ONE producer --> MANY consumers"
+++ не проходил ext.test (что-то с sql fadeout)
- сделать реализацию AsyncMongo для замены MySQL 
    (!! там не особо понятно, чем заменить проверку по "активному серверу" -- видимо, придется в ISql подменять обработку, и смотреть по peer servers своего WS)

(FUTURE) ? sync-варианты для работы с mc/sql/mongo?
(FUTURE) - порефакторить cluster_*, выделить общего предка AsyncClusterQueue, и от него уже вывести все асинк-хэндлеры (думаю, будут еще изменения работы с queue)
+++ механизм переподключения к sql-базам (завершить накопленные запросы к старым базам, доложить, подключиться к новым базам?)


=================================================================================
Use case
•	Social клиент авторизируется
•	В social клиенте пользователь нажимает “хочу поиграть”
•	Выбирает героя (сторону ?) 
•	На экране возникает Контрол отображающий прогресс поиска партии
•	Social клиент отправляет запрос на social server
•	Social сервер возвращает клиенту id по которому клиент может получать информацию о прогрессе
•	Social сервер формирует запрос на matchmaking (включая текущий рейтинг игрока, его надежность и другую persistent информацию)
•	Matchmaking сервер периодически забирает запросы на matchmaking и начинает их обрабатывать
•	По мере выполнения запросов информация приходит на social сервер
•	Social клиент периодически запрашивает статус matchmaking
•	Social клиент получает информацию о найденной сессии и стартует battleground клиента, передавая ему эту информацию + информацию об авторизации
•	Battleground клиент получая информацию о сессии, коннектится на Battleground сервер


Цель собрания
•	Выработать диаграмму активности 
•	Разбить use case на задачи
•	Обсудить вопрос с передачей информации об авторизации между частями  
=================================================================================


+ развернуть тестовый кластер в Amazon EC2
+++ потестировать скорости mc/sql/mongo (между двумя EC2-инстансами)
+++ потестировать скорости mc/sql/mongo на localhost (внутри одного EC2-instance)

+ сделать в тестах механизм для отработки async запросов через IOLoop 
+ сделать пару новых заглушек (уметь генерить ответ "юзер есть на соседнем сервере" для peer messaging)

+ завести dummy-интерфейсы для memcache-layer и sql-layer (завернуть hardcoded-запросы в методы типа mc.CheckLogin, sql.Login, и т.п., сделать тривиальную реализацию)
(4ч)
+ нарисовать картинки (4ч)
+ расписать deployment (3ч)
++ желательно еще расписать конфиг-опции (nginx, coord_cfg) и опции командной строки (ws, coordinator, mc)

+ РЕФАКТОРИНГ КЛАССА JsonHandler: 
+++ уйти от синглтонов для MC/SQL, передавать эти обработчики в конструктор ReqHandler как доп. параметры
--~ порефакторить и координатор: всякие tick, checkWebServers и т.п. унести в class Coordinator

+++>> легко делать unit test-ы 

+++ вынести login и give во внешние файлы (как примеры "внешних handler'ов")
+++ легко расширять обработку новых запросов "внешними handler'ами" 
    (напр. подкладывать /extensions/***Handler.py + /extensions/__init__.py)


CURRENT
+ проверять доступность БД-шардов, ругаться
+ игнорировать MC-лок дохлого сервера (имеет смысл поменять WS peers[name] на peers[server_id]?)
- придумать и написать тест на "отсутствие одновременного логина на разных WS" 
+++ например, при логине пишем в БД-лог (uid, server_id, "login", время логина); при логауте ("off"); при смерти сервера пишем (0, server_id, "shutdown", время)
    (и потом следим, чтобы не было "login server2" между "login server1" и "off server1")
----- BUG: к сожалению, на локале тестовый клиент с вменяемым кол-вом threads заваливает async-http систему нафиг (времена ответа становятся порядка нескольких секунд)
+++ заодно хотелось бы, чтобы в тесте использовались и peer-операции, и сериализация в БД

+ сделать вариант self-message без реального http-коннекта (как-нибудь замкнуть RequestHandler напрямую, а то многовато накладных расходов на headers, connect и т.п.)
    (1) можно решить вопросы с self.getParam, self.fin; как-то обернуть вложенный self.response -- тогда можно делать любые прямые вызовы
    (2) можно выделить "внутренний layer" исполнения запроса: onEvent выдирает параметры, ставит self._fin = fin() и вызывает onEvent_1; onEvent_N вызывает self._fin;
    -> сделал отдельную обертку SubEvent, которой даем arguments и callback, и вызываем искомый метод (напр. subhandler.onLoginRequest)


FUTURE
? logout по команде
? challenge авторизации: шлем клиенту случайный key, он присылает md5(xxx + key); после этого считаем юзера авторизованным (для Facebook, впрочем, это излишне)

FUTURE "COORDINATOR (2)"
? генерить nginx-конфиг и перезапускать nginx ?
? выдача WS-ке следующего имени/порта (тогда можно запускать WS без всяких параметров, просто с адресом координатора)
? механизм переподключения к sql-базам (завершить накопленные запросы к старым базам, доложить, подключиться к новым базам?)
--- подключиться к новым базам, кстати, можно сразу: убираем старые loop.sql[] из ioloop, но при этом старые AsyncSqlThread'ы спокойно доразберут уже запощенные query[]
+++++ надо только научить sql threads останавливаться по некоторому признаку (им стало наличие полей owner.stopSQL/owner.fadeoutSQL)


===================================
ЧТО ЕСТЬ
===================================
- кластер: nginx load balancer, WebServers, MemCache, MySQL, coordinator
- WS: login/logout, keepalive, self/peer message
- координатор: следит за состоянием WS/MC/SQL, апдейтит WS-кам конфиг кластера (состояние+адреса WS/MC/SQL) 
{общение с MC умеем реконфигурировать на лету, SQL и load balancing пока нет}
- тесты: есть простые нагрузочные batch-тесты (login + keepalive) с несколькими разными сценариями (разные http-библиотеки, разные режимы нагрузки)


===================================
НАПРАВЛЕНИЯ:
===================================
= надежность и выживаемость (возможности конфигурации на лету): WS имена/порты на координатор, sql config, nginx config
= секьюрность протокола: авторизация, facebook auth/signature, auth challenge (возможно, периодический)
= игровая логика (данные героев/сессий, маршаллинг бд -> социалка -> pvp)
= тесты и облагораживание интерфейсов (+вынос sql/mc операций в интерфейсы, компонентный unit testing)


===================================
DEPLOYMENT
===================================
nginx
- PCRE
- zlib
> config
memcache
> batch files
python 2.6
- libcurl/pycurl
- MySQLdb
> install svn
> config.py, batch params (coordinator, wserver, memcache/sql addresses)
MySQL
> apply pw.sql

1) в варианте без MC/SQL/nginx: ставим python 2.6, libcurl/pycurl, mysqldb (import-то все равно будет требовать pycurl+mysqldb), 
    сливаем исходники, полетели через standalone*.bat 
2) nginx и memcache под виндой нормально живут прямо в виде exe-шников из SVN
3) вот с MySQL базами придется повозиться 
- поставить MySQL
- юзером "pw" завести базы (MYISAM, c DEFAULT CHARSET utf8)
- накатить pw.sql + завести N тестовых логинов (можно делать по спец. ключику где-нибудь в координаторе: "--sql=async --create_test_users=1000")

===================================
механизм передачи вещи между Work Servers (A)
===================================
- простой message list, работающий через memcache ИЛИ обмен между WS-ками по http (полная звезда), вероятно через tornado.AsyncHttpClient
- ответы придется, видимо, обрабатывать по timeout
--- синхронно с отправкой в MC заносим в messages[], периодически пробегаемся, фейлим мессаги по timeout'ам
?? как вариант, можно подумать насчет локальных http-коннектов (аналогично координатору), или вообще об общении через координатор.. но появляется Single P.O.F.

сам механизм "передачи item":
- выясняем, где target-юзер; если свободен, подтягиваем к себе, и выполняем все локально
-1a- шлем peer_message(GIVE_CHECK, target_uid, item, itew_ws_guid=сгенеренное уникальное имя вещи, вида "one_12345")
-1b- получаем ответ(GIVE_ACCEPT), отправляем GIVE
-1c- получаем ответ(GIVE_OK), удаляем у себя оригинал вещи
-1с'- если ответа GIVE_OK нет (timeout), повторяем GIVE до легкого утомления

-2a- получаем GIVE_CHECK, кладем себе item[place_holder, timeout, item_ws_guid], шлем GIVE_ACCEPT
-2b- получаем GIVE, снимаем с копии item флаг place_holder, шлем GIVE_OK

! стадия -1c-, увы, критична: если не получили ответ, имеем dupe вещи.
--- ну и хрен с ним. для предотвращения дюпа Реальных Артефактов - заводим им уникальное имя в мире, и отдельную таблицу в БД. операции, соотв., исполняем через БД.

============================================
НАДЕЖНАЯ схема передачи вещи (B) может выглядеть так:
============================================
=1b= получаем GIVE_ACCEPT (адресат готов принять вещь), и перед тем как отправить GIVE, выполняем транзакции в БД:
- пишем в SQL(sender_uid) ticket (send, itew_ws_guid, target_uid)
- пишем в SQL(target_uid) ticket (recv, itew_ws_guid, полная инфа вещи) !! с этого момента транзакция считается выполненной, 
    и любое восстановление должно привести к получению вещи адресатом и изъятию вещи у отправителя.
- выкидываем вещь у себя, выполняем сохранение вещей(sender_uid) с указанием ticket(send itew_ws_guid) как исполненного (при этом ticket в sender-БД должен зачиститься в процедуре сохранения)
- отправляем GIVE (при необходимости несколько раз, -1c-/-1c'-)

=2b= получив GIVE (с указанным item_ws_guid), кладем себе вещь и выполняем сохранение вещей, с указанием ticket(recv item_ws_guid) как исполненного (при этом ticket в target-БД должен зачиститься).

==== при гибели sender WS/БД в процессе исполнения: 
- загрузка увидит наличие ticket(send itew_ws_guid)
- проверит наличие в target-БД ticket(recv item_ws_guid)
- дальше перейдем к исполнению прерванного процесса GIVE

==== при гибели target WS/БД в процессе исполнения (после того как sender уже исполнил свою часть)
- загрузка увидит наличие ticket(recv item_ws_guid)
- выдадим юзеру вещь; ticket(recv) пометим как неактивный, но удалять не будем -- это сделаем только при получении GIVE с уже выполненными реквизитами (т.к. после нас может переподняться sender WS, и пытаться исполнить удаление вещи, с "проверкой наличия в target-БД ticket(recv)"

----------------------------------------------------------
!!! в реальности пользоваться такой схемой - адский геморрой, и масса лишних запросов к БД (4 запроса на одну нормальную передачу вещи)
гораздо проще допустить мизерную вероятнось DUPE при серьезных авариях (падение серверов/БД), и пользоваться схемой (A).
----------------------------------------------------------
для учета _небольшого_ числа транзакций (с какими-нибудь убер-вещами) можно пользоваться центральной master-БД;
но это потенциально херит масштабируемость (как минимум, загрузку из центральной БД надо делать при старте (или периодическом тике) WS, 
    а не при логине конкретного юзера - иначе задавим единичную мастер-БД запросами)
----------------------------------------------------------


=======================
ПОКУПКА вещи (биллинг)
=======================
с нашей стороны:
-1- шлем в биллинг BUY(uid, item_type, бабло..)
-2- биллинг заводит строчку транзакции (с флагом confirmed=0), отвечает BUY_OK(transaction_id, uid, item, новое бабло..)
-3- сохраняем добро в БД (строчку биллинг-операции для отчетности + вещи игрока)
-4- после ответа БД (save ok) шлем в биллинг BUY_CONFIRM(uid, transaction_id..), биллинг ставит в соотв. строчке флаг confirm=1
для неподтвержденных транзакций возможны запросы (на релогине и т.п.) "дайте неподтвержденные по юзеру uid" или (периодически) "дайте все неподтвержденные"

если инициатива с внешней стороны (покупка на сайте и т.п.), начинаем со стадии -2- (BUY_CONFIRM становится ответом на запрос со стороны биллинга BUY_OK)


=================================
координация (на случай смерти WS)
=================================
+ сделать в БД табличку "активных серверов", учитывать ее содержимое в sp_login
+!! по идее, писать в эту табличку должен координатор (причем копия таблички должна быть в каждом БД-шарде, и именно координатор отвечает за синхронизацию всех копий)

[+ программа-минимум 1]
+ проверять WS по списку (по умолчанию они, например, "мертвые")
+ когда для живого WS http-запрос failed, во всех БД делать "DELETE FROM tbl_servers WHERE id=server_id"

[+ программа-минимум 2]
+ WS, поднимаясь, стучится к координатору на его http-порт (именно это сигнал "сделать INSERT INTO tbl_servers и известить всех WS")

+ координатор, поднимаясь, тоже должен проверить WS-сервера по списку (python config?), и если найдутся живые, подгрести их в кластер без всяких ресетов 
    (вероятно, это как раз координатор сдох и переподнимается)	
+ вынести конфиг кластера в отдельный внешний питоновский cfg.py (? или в какую-нибудь master БД ?)
- желательно проверять и по доступным БД-шардам (туда в перспективе могут динамически вписаться WS, не прописанные в python config)

(?) ключ логина в MC выставлять на ограниченное время (тогда, увы, придется его апдейтить :( 
+ (в MC или) WS тоже раздавать список активных/мертвых серверов (этим может заниматься координатор), локи мертвых серверов можно игнорировать
---? сам мертвый сервер должен откуда-то узнавать о своей смерти? (варианты: от координатора / из MC / из БД)
----- в БД операции "официально мертвого сервера" должны, вероятно, фейлиться -- да, оверхед на tbl_servers, конечно; но надежно (?..)

- lazy очередь нельзя "совсем на второй план", до нее может тогда вообще не доходить дело; 
    лучше ставить пропорцию, в которой берем main:lazy (напр. 10:1)
    
- предусматривать возможность смерти БД (логины и прочие запросы обламываем)
--- а вот что делать с юзерами "уже на сервере"? - не удалять, видимо; сливать куда-то в IDLE

=================================
WS STARTUP:
=================================
+! сам координатор тоже может когда-нибудь сдохнуть и переподняться
+ где-то координатор хранит таблички с описанием компонент - может, просто у себя внутри (в coord_cfg.py)
    { server_id, server_name, port }  -- для nginx, кстати, принципиальны только имя и порт, server_id можно инкрементить произвольно
    { mc_id,  ip:port }
    { sql_id, ip:port, base, pwd }
+ WS при запуске знает [ТОЛЬКО?] http-адрес coordinator-а; шлет ему http-request WS_START: типа "я новый сервер, дай мои реквизиты и весь конфиг"
//?? координатор заводит новую запись в мастер-БД (получает server_id); 
+++ координатор апдейтит все БД-шарды (прописывает новый "активный сервер")
+++ после завершения всех этих дел (синк) координатор, наконец, отвечает новому серверу на WS_START:
    сообщает его реквизиты (на каком порту слушать, и т.п.)
+++ новый сервер радостно продолжает init, начинает слушать, и после этого шлет координатору WS_READY (готов к нагрузке)
+++ координатор сообщает новенькому весь кластерный конфиг (MC, SQL, other WS), ответом на WS_READY
--- координатор генерит новый nginx-конфиг, и "nginx -reload"  ==> чтобы nginx начинал балансить новый сервер
+++ (в ближайший tick после WS_READY:) координатор сообщает всем уже живым серверам, что появляется новый живой (и дает готовый ip:port, на котором новенький уже слушает)

=================================
WS FAIL:
=================================
+ координатор периодически опрашивает все WS (по http)
+ когда WS несколько раз не отвечает на http (констатируется до-о-олгий timeout), координатор решает его прибить.
--- эти же сообщения (с порядковым номером "предупреждения") могут служить WS-ке ориентиром, что надо как можно скорее избавляться от лишних текущих юзеров,
    и по возможности отказывать новым (отдавать какой-нибудь error 50x "прием ограничен")

когда констатировали, что WS не отвечает, в каком-то порядке:
- сгенерить nginx.conf (уже без дохлого WS_X) и "nginx -reload" ==> nginx перестанет балансить сервер, все клиентские запросы к нему обломятся 
--- возможно, после этого WS довольно быстро воспрянет (прекратится поток incoming actions, разгребутся завалы)

//(?) убрать WS_X из мастер-БД (с этого момента процесс WS remove считаем необратимым)
++ убрать WS_X из БД-шардов ==> начнут оверрайдиться его БД-локи
++ сказать всем остальным WS, что сервер WS_X мертв (заодно лишний раз сказать и самому WS_X, вдруг проскочит) ==> начнут оверрайдиться его MC-локи


=================================
тест-клиент
=================================
+ сценарий 1: создаем нитку с генерацией http-запросов (произвольный id, произвольный server, произвольный action)
+ сценарий 1: создаем нитку, которая правильно логинится, смотрит на ответ, затем периодически шлет правильный keepalive


===================================================================================================
LOGIN
===================================================================================================
[IN] http-запрос клиента
    action=login
    username=<string>
    pwd=<md5>
[OUT] ответ
    success=1
        uid=<int>
        key=<int>
        server=<string> // наш server_name (типа "one", "two" и т.п.)
            error="already logged" // возможен, когда мы уже залогинены на данный сервер 
    success=0 
        error="bad login or pwd" 
        error="already logged" 
            server=<string> // server_name другого сервера (типа "one", "two" и т.п.), где юзер залоган
    
1. проверяем наличие у себя [login] -> id, md5, ws
нету: 
    2. ломимся в MC
        [login] -> id, md5, ws // причем ws != self.ws это признак отлупа "already logged на другом сервере"
    есть, свободный (ws == 0):
        отвечаем юзеру
        4. ломимся в MC, ставим ws=self.ws 
        ! НУЖЕН КАКОЙ-ТО СВОЙ ЛОК, ЧТОБЫ ДЕЛАТЬ CHECK_WS/SET_WS АТОМАРНО: делаем gets/cas			
        и все равно делаем 3. (хотя можем ставить LOW PRIORITY и сквозь пальцы смотреть на ответ)
    есть, свой (ws == self.ws):
        ASSERT (н.е.х., почему это мы сами не знаем, что юзер нащ)
    есть, чужой:
        отлуп
    нету:
        3. ломимся в БД
            [CALL] CALL sp_login( login, self.ws )
            [RET] id, login, md5, ws // причем ws != self.ws это признак отлупа "already logged на другом сервере"


--------------------------------------------------------------------------------------------------------

1) Load Balancer (nginx)
+ поставить
+ разобраться с конфигом, настроить для 2 серверов ("/one", "/two", "/*")

2) 2x Web Servers (tornado)
+ поставить
+ настроить для 2 серверов (one/two)
+ написать прототипы: 
+++ обработка команды пользователя (http), 
+++ общение с memcache, 
+++ общение с sql

3) 2x MemCached
+ поставить (mc_win + python "memcache" lib)
+ настроить (2 порта)
+ наладить группировку ключей (uid->ws и uid->data должны попадать на один mc server)
! в любом случае надо обертывать в thread-pool-оболочку, скрывающую детали конкретной кэш-системы (может меняться на Redis или что-то еще)
--- построение запросов тогда тоже надо скрыть за вирт. методами оболочки: 
    check_login(name, ws) -> ws, cas_id = "gets"(login), "cas"(login, cas_id, ws) и т.п.

4) 2x postgres/MySQL DataBase 
+ поставить (postgres DB + python "psycopg" lib)
+ настроить 2 базы с одинаковыми таблицами 
++- tbl_login: id, login, pwd
+-- tbl_brick: id, ver, serverId, data
! в любом случае надо обертывать в thread-pool-оболочку, скрывающую детали конкретной БД (может меняться)
--- построение запросов тогда тоже надо скрыть за вирт. методами оболочки:
    get_login(name, ws) -> "CALL sp_login(%r, %d)" и т.п.)
? возможно есть еще backup Database (туда льем, когда "правильная" по настройкам DB почему-то offline)

--------------------------------------------------------------------------------------------------------

5) Coordinator guard (generic python)
+ периодически ломится на WS (наверно, прямо по http, на конкретный порт), 
-- просит статистику, меряет latency
+ периодически ломится на все MC, проверяет что создаются (и читаются?) его (гарда) личные ключики, 
-- меряет latency
+ периодически ломится во все DB, проверяет что (создаются?) читаются его (гарда) личные записи, меряет latency

восстановление жизнедеятельности:
+ если отказал WS, нужно зачистить все его локи в MC и DB (соотв. user-контексты становятся доступны другим WS)
+ если отказал один из MC, нужно апдейтнуть список MC-серверов у всех WS-ок 
--- и попросить заново acquire в "переконфигуренном MC" все свои текущие локи
- если отказала одна из DB, хорошо бы перенаправить соотв. запись в backup-DB (при восстановлении пригодится), а чтение исправно фейлить 
    (этот сегмент юзеров становится недоступен для логина и операций)

расширение:
+ если добавился WS, ничо не нужно делать 
--- только апдейтнуть конфиг nginx и reload его, дальше новый WS постепенно получит свою долю юзеров
--- кстати, прочие WS могут смотреть общую нагрузку (по статистике от координатора) и при переработке начинать планомерно "отлынивать", 
    отпуская бОльшую долю своих юзер-контекстов в общий доступ
+ если добавился MC, апдейтим всем WS-кам список MC-серверов 
--- краткосрочно будет много фейлов типа "cache miss", поэтому можно попросить все WS специально посливать свежую инфу в MC
- если добавилась DB (тут лучше всегда делать x2 кол-во серверов, чтобы гладко почковались записи юзеров), нужно апдейтнуть список DB у всех WS,
    и посоздавать у них новые пулы DB connections

